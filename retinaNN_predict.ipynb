{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Python\n",
    "#import module for predict\n",
    "import os\n",
    "import numpy as np\n",
    "import configparser\n",
    "from matplotlib import pyplot as plt\n",
    "#Keras\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model\n",
    "#scikit learn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "import sys\n",
    "sys.path.insert(0, './lib/')\n",
    "# help_functions.py\n",
    "from help_functions import *\n",
    "# extract_patches.py\n",
    "from extract_patches import recompose_img\n",
    "from extract_patches import recompose_overlap_img\n",
    "from extract_patches import paint_border\n",
    "from extract_patches import kill_border\n",
    "from extract_patches import pred_only_FOV\n",
    "from extract_patches import get_data_testing\n",
    "from extract_patches import get_data_testing_overlap\n",
    "# pre_processing.py\n",
    "from pre_processing import my_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= CONFIG FILE TO READ FROM =======\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('configuration.txt')\n",
    "\n",
    "#run the training on invariant or local\n",
    "path_data = config.get('data paths', 'path_local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DRIVE_datasets_training_testing/\n"
     ]
    }
   ],
   "source": [
    "print(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original test images (for FOV selection)\n",
    "# init setting (PATH, mode, parameter)\n",
    "DRIVE_test_imgs_original = path_data + config.get('data paths', 'test_imgs_original')\n",
    "test_imgs_orig = load_hdf5(DRIVE_test_imgs_original)\n",
    "full_img_height = test_imgs_orig.shape[2]\n",
    "full_img_width = test_imgs_orig.shape[3]\n",
    "#the border masks provided by the DRIVE\n",
    "DRIVE_test_border_masks = path_data + config.get('data paths', 'test_border_masks')\n",
    "test_border_masks = load_hdf5(DRIVE_test_border_masks)\n",
    "\n",
    "# gtruth\n",
    "DRIVE_gtruth= path_data + config.get('data paths', 'test_groundTruth')\n",
    "\n",
    "# dimension of the patches\n",
    "patch_height = int(config.get('data attributes', 'patch_height'))\n",
    "patch_width = int(config.get('data attributes', 'patch_width'))\n",
    "#the stride in case output with average\n",
    "stride_height = int(config.get('testing settings', 'stride_height'))\n",
    "stride_width = int(config.get('testing settings', 'stride_width'))\n",
    "assert (stride_height < patch_height and stride_width < patch_width)\n",
    "#model name\n",
    "name_experiment = config.get('experiment name', 'name')\n",
    "path_experiment = './' +name_experiment +'/'\n",
    "#N full images to be predicted\n",
    "Imgs_to_test = int(config.get('testing settings', 'full_images_to_test'))\n",
    "#Grouping of the predicted images\n",
    "N_visual = int(config.get('testing settings', 'num_group_visual'))\n",
    "#====== average mode ===========\n",
    "average_mode = config.getboolean('testing settings', 'average_mode')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DRIVE_datasets_training_testing/DRIVE_dataset_imgs_test.hdf5\n"
     ]
    }
   ],
   "source": [
    "print(DRIVE_test_imgs_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[group images func] prev data shape  : (20, 3, 584, 565)\n",
      "[group images func] after data shape :  (20, 584, 565, 3)\n",
      "[group images func] first total image :  (584, 2825, 3)\n",
      "[group images func] final total image :  (2920, 2825, 3)\n",
      "data shape :  (2920, 2825, 3)\n",
      "<PIL.Image.Image image mode=RGB size=2825x2920 at 0x7F6D100232B0>\n",
      "file name :  ./DRIVE_test_norm_190000/imgs_test\n",
      "[group images func] prev data shape  : (20, 1, 584, 565)\n",
      "[group images func] after data shape :  (20, 584, 565, 1)\n",
      "[group images func] first total image :  (584, 2825, 1)\n",
      "[group images func] final total image :  (2920, 2825, 1)\n",
      "data shape :  (2920, 2825, 1)\n",
      "<PIL.Image.Image image mode=L size=2825x2920 at 0x7F6C511D7470>\n",
      "file name :  ./DRIVE_test_norm_190000/imgs_test_border\n",
      "[group images func] prev data shape  : (20, 1, 584, 565)\n",
      "[group images func] after data shape :  (20, 584, 565, 1)\n",
      "[group images func] first total image :  (584, 2825, 1)\n",
      "[group images func] final total image :  (2920, 2825, 1)\n",
      "data shape :  (2920, 2825, 1)\n",
      "<PIL.Image.Image image mode=L size=2825x2920 at 0x7F6C511D7908>\n",
      "file name :  ./DRIVE_test_norm_190000/imgs_test_ground\n"
     ]
    }
   ],
   "source": [
    "# visualizing\n",
    "test_img_ori = load_hdf5(DRIVE_test_imgs_original)\n",
    "test_img_border = load_hdf5(DRIVE_test_border_masks)\n",
    "test_img_grd = load_hdf5(DRIVE_gtruth)\n",
    "\n",
    "temp_tot_test1 = visualize(group_images(test_img_ori[0:20,:,:,:],5),'./'+name_experiment+'/imgs_test')\n",
    "temp_tot_test2 = visualize(group_images(test_img_border[0:20,:,:,:],5),'./'+name_experiment+'/imgs_test_border')\n",
    "temp_tot_test3 = visualize(group_images(test_img_grd[0:20,:,:,:],5),'./'+name_experiment+'/imgs_test_ground')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3, 584, 565)\n",
      "type :  <class 'numpy.ndarray'>\n",
      "[get data testing overlap] prev test img shape : (20, 1, 584, 565) ground truth shape : (20, 1, 584, 565) \n",
      "\n",
      "the side H is not compatible with the selected stride of 5\n",
      "img_h 584, patch_h 48, stride_h 5\n",
      "(img_h - patch_h) MOD stride_h: 1\n",
      "So the H dim will be padded with additional 4 pixels\n",
      "\n",
      "the side H is not compatible with the selected stride of 5\n",
      "img_h 565, patch_h 48, stride_h 5\n",
      "(img_h - patch_h) MOD stride_h: 2\n",
      "So the H dim will be padded with additional 3 pixels\n",
      "new full images shape: \n",
      "(20, 1, 588, 568)\n",
      "\n",
      "the side H is not compatible with the selected stride of 5\n",
      "img_h 584, patch_h 48, stride_h 5\n",
      "(img_h - patch_h) MOD stride_h: 1\n",
      "So the H dim will be padded with additional 4 pixels\n",
      "\n",
      "the side H is not compatible with the selected stride of 5\n",
      "img_h 565, patch_h 48, stride_h 5\n",
      "(img_h - patch_h) MOD stride_h: 2\n",
      "So the H dim will be padded with additional 3 pixels\n",
      "new full images shape: \n",
      "(20, 1, 588, 568)\n",
      "[get data testing overlap] after test img shape : (20, 1, 588, 568) ground truth shape : (20, 1, 588, 568) \n",
      "[get_data_testing_overlap func] test images range (min-max): 0.0 - 1.0\n",
      "[get_data_testing_overlap fucn] test masks are within 0-1\n",
      "\n",
      "extract , :  540 5 520 5\n",
      "[extrct_order_overlap func] Number of patches on h : 109\n",
      "[extrct_order_overlap func] Number of patches on w : 105\n",
      "[extrct_order_overlap func] number of patches per image: 11445 totally for this dataset: 228900\n",
      "\n",
      "[get_data_testing_overlap func] test PATCHES images shape:\n",
      "(228900, 1, 48, 48)\n",
      "[get_data_testing_overlap func] test PATCHES images range (min-max): 0.0 - 1.0\n"
     ]
    }
   ],
   "source": [
    "#============ Load the data and divide in patches\n",
    "patches_imgs_test = None\n",
    "new_height = None\n",
    "new_width = None\n",
    "masks_test  = None\n",
    "patches_masks_test = None\n",
    "if average_mode == True:\n",
    "    patches_imgs_test, new_height, new_width, masks_test = get_data_testing_overlap(\n",
    "        DRIVE_test_img_ori_path = DRIVE_test_imgs_original,  #original\n",
    "        DRIVE_test_img_grd_path = path_data + config.get('data paths', 'test_groundTruth'),  #masks\n",
    "        num_test_img = int(config.get('testing settings', 'full_images_to_test')),\n",
    "        patch_h = patch_height,\n",
    "        patch_w = patch_width,\n",
    "        stride_h = stride_height,\n",
    "        stride_w = stride_width\n",
    "    )\n",
    "else:\n",
    "    patches_imgs_test, patches_masks_test = get_data_testing(\n",
    "        DRIVE_test_img_ori_path = DRIVE_test_imgs_original,  #original\n",
    "        DRIVE_test_img_grd_path = path_data + config.get('data paths', 'test_groundTruth'),  #masks\n",
    "        num_test_img = int(config.get('testing settings', 'full_images_to_test')),\n",
    "        patch_height = patch_height,\n",
    "        patch_width = patch_width,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DRIVE_test_norm_190000/DRIVE_test_norm_190000_architecture.json\n"
     ]
    }
   ],
   "source": [
    "print(path_experiment +name_experiment+'_architecture.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228900, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(patches_imgs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ Run the prediction of the patches ==================================\n",
    "best_last = config.get('testing settings', 'best_last')\n",
    "#Load the saved model\n",
    "model = model_from_json(open(path_experiment +name_experiment+'_architecture.json').read())\n",
    "model.load_weights(path_experiment+'/'+best_last+'_weights.h5')\n",
    "#Calculate the predictions\n",
    "predictions = model.predict(patches_imgs_test, batch_size=64, verbose=2)\n",
    "print (\"predicted images size :\",predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===== Convert the prediction arrays in corresponding images\n",
    "pred_patches = pred_to_imgs(predictions, patch_height, patch_width, \"original\")\n",
    "pred_patches_thr = pred_to_imgs(predictions, patch_height, patch_width, \"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== Elaborate and visualize the predicted images ====================\n",
    "pred_imgs = None\n",
    "orig_imgs = None\n",
    "gtruth_masks = None\n",
    "\n",
    "if average_mode == True:\n",
    "    pred_imgs = recompose_overlap_img(pred_patches, new_height, new_width, stride_height, stride_width)# predictions\n",
    "    pred_imgs_thr = recompose_overlap_img(pred_patches_thr, new_height, new_width, stride_height, stride_width)# predictions\n",
    "\n",
    "    orig_imgs = my_preprocessing(test_imgs_orig[0:pred_imgs.shape[0],:,:,:])    #originals\n",
    "    gtruth_masks = masks_test  #ground truth masks\n",
    "    \n",
    "\n",
    "else:\n",
    "    pred_imgs = recompose_img(pred_patches,13,12)       # predictions\n",
    "    pred_imgs_thr = recompose_img(pred_patches_thr,13,12)       # predictions\n",
    "    orig_imgs = recompose_img(patches_imgs_test,13,12)  # originals\n",
    "    gtruth_masks = recompose_img(patches_masks_test,13,12)  #masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the DRIVE masks on the repdictions #set everything outside the FOV to zero!!\n",
    "kill_border(pred_imgs, test_border_masks)  #DRIVE MASK  #only for visualization\n",
    "## back to original dimensions\n",
    "orig_imgs = orig_imgs[:,:,0:full_img_height,0:full_img_width]\n",
    "pred_imgs = pred_imgs[:,:,0:full_img_height,0:full_img_width]\n",
    "gtruth_masks = gtruth_masks[:,:,0:full_img_height,0:full_img_width]\n",
    "print (\"Orig imgs shape: \" +str(orig_imgs.shape))\n",
    "print (\"pred imgs shape: \" +str(pred_imgs.shape))\n",
    "print (\"Gtruth imgs shape: \" +str(gtruth_masks.shape))\n",
    "\n",
    "visualize(group_images(orig_imgs,N_visual),path_experiment+\"all_originals\")#.show()\n",
    "visualize(group_images(pred_imgs,N_visual),path_experiment+\"all_predictions\")#.show()\n",
    "visualize(group_images(gtruth_masks,N_visual),path_experiment+\"all_groundTruths\")#.show()\n",
    "#visualize results comparing mask and prediction:\n",
    "assert (orig_imgs.shape[0]==pred_imgs.shape[0] and orig_imgs.shape[0]==gtruth_masks.shape[0])\n",
    "N_predicted = orig_imgs.shape[0]\n",
    "group = N_visual\n",
    "assert (N_predicted%group==0)\n",
    "\n",
    "result_path = 'each_results'\n",
    "if os.path.isdir(path_experiment+result_path) == False:\n",
    "    os.mkdir(path_experiment +result_path)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format(path_experiment +result_path))\n",
    "\n",
    "for i in range(int(N_predicted/group)):\n",
    "    orig_stripe = group_images(orig_imgs[i*group:(i*group)+group,:,:,:],group)\n",
    "    masks_stripe = group_images(gtruth_masks[i*group:(i*group)+group,:,:,:],group)\n",
    "    pred_stripe = group_images(pred_imgs[i*group:(i*group)+group,:,:,:],group)\n",
    "    total_img = np.concatenate((orig_stripe,masks_stripe,pred_stripe),axis=0)\n",
    "    \n",
    "    visualize(pred_stripe,path_experiment+ result_path +'/'+\"_Prediction\"+str(i))#.show()\n",
    "    visualize(total_img,path_experiment+ result_path +'/'+\"_Original_GroundTruth_Prediction\"+str(i))#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores, y_true = pred_only_FOV(pred_imgs, gtruth_masks, test_border_masks)\n",
    "print (\"Calculating results only inside the FOV:\")\n",
    "print (\"y scores pixels: \" +str(y_scores.shape[0]) +\" (radius 270: 270*270*3.14==228906), including background around retina: \" +str(pred_imgs.shape[0]*pred_imgs.shape[2]*pred_imgs.shape[3]) +\" (584*565==329960)\")\n",
    "print (\"y true pixels: \" +str(y_true.shape[0]) +\" (radius 270: 270*270*3.14==228906), including background around retina: \" +str(gtruth_masks.shape[2]*gtruth_masks.shape[3]*gtruth_masks.shape[0])+\" (584*565==329960)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "#roc curve\n",
    "fpr, tpr, thresholds = roc_curve((y_true), y_scores)\n",
    "auc_roc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "print (\"\\nArea under the ROC curve: \" +str(auc_roc))\n",
    "roc_curve = plt.figure()\n",
    "\n",
    "plt.plot(fpr,tpr,'-',label='Area Under the Curve (AUC = %0.4f)' % auc_roc)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel(\"FPR (False Positive Rate)\")\n",
    "plt.ylabel(\"TPR (True Positive Rate)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(path_experiment+\"ROC.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "precision = np.fliplr([precision])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "recall = np.fliplr([recall])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "AUC_prec_rec = np.trapz(precision,recall)\n",
    "print (\"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec))\n",
    "prec_rec_curve = plt.figure()\n",
    "\n",
    "plt.plot(recall,precision,'-',label='Area Under the Curve (AUC = %0.4f)' % AUC_prec_rec)\n",
    "plt.title('Precision - Recall curve')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(path_experiment+\"Precision_recall.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "threshold_confusion = 0.5\n",
    "print (\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
    "y_pred = np.empty((y_scores.shape[0]))\n",
    "for i in range(y_scores.shape[0]):\n",
    "    if y_scores[i]>=threshold_confusion:\n",
    "        y_pred[i]=1\n",
    "    else:\n",
    "        y_pred[i]=0\n",
    "        \n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "print (confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "if float(np.sum(confusion))!=0:\n",
    "    accuracy = float(confusion[0,0]+confusion[1,1])/float(np.sum(confusion))\n",
    "print (\"Global Accuracy: \" +str(accuracy))\n",
    "\n",
    "specificity = 0\n",
    "if float(confusion[0,0]+confusion[0,1])!=0:\n",
    "    specificity = float(confusion[0,0])/float(confusion[0,0]+confusion[0,1])\n",
    "print (\"Specificity: \" +str(specificity))\n",
    "sensitivity = 0\n",
    "if float(confusion[1,1]+confusion[1,0])!=0:\n",
    "    sensitivity = float(confusion[1,1])/float(confusion[1,1]+confusion[1,0])\n",
    "print (\"Sensitivity: \" +str(sensitivity))\n",
    "precision = 0\n",
    "if float(confusion[1,1]+confusion[0,1])!=0:\n",
    "    precision = float(confusion[1,1])/float(confusion[1,1]+confusion[0,1])\n",
    "print (\"Precision: \" +str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Jaccard similarity index\n",
    "jaccard_index = jaccard_similarity_score(y_true, y_pred, normalize=True)\n",
    "print (\"\\nJaccard similarity score: \" +str(jaccard_index))\n",
    "\n",
    "#F1 score\n",
    "F1_score = f1_score(y_true, y_pred, labels=None, average='binary', sample_weight=None)\n",
    "print (\"\\nF1 score (F-measure): \" +str(F1_score))\n",
    "\n",
    "#Save the results\n",
    "file_perf = open(path_experiment+'/performances.txt', 'w')\n",
    "file_perf.write(\"Area under the ROC curve: \"+str(auc_roc)\n",
    "                + \"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec)\n",
    "                + \"\\nJaccard similarity score: \" +str(jaccard_index)\n",
    "                + \"\\nF1 score (F-measure): \" +str(F1_score)\n",
    "                +\"\\n\\nConfusion matrix:\"\n",
    "                +str(confusion)\n",
    "                +\"\\nACCURACY: \" +str(accuracy)\n",
    "                +\"\\nSENSITIVITY: \" +str(sensitivity)\n",
    "                +\"\\nSPECIFICITY: \" +str(specificity)\n",
    "                +\"\\nPRECISION: \" +str(precision)\n",
    "                )\n",
    "file_perf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the DRIVE masks on the repdictions #set everything outside the FOV to zero!!\n",
    "kill_border(pred_imgs_thr, test_border_masks)  #DRIVE MASK  #only for visualization\n",
    "## back to original dimensions\n",
    "orig_imgs = orig_imgs[:,:,0:full_img_height,0:full_img_width]\n",
    "pred_imgs_thr = pred_imgs_thr[:,:,0:full_img_height,0:full_img_width]\n",
    "gtruth_masks = gtruth_masks[:,:,0:full_img_height,0:full_img_width]\n",
    "print (\"Orig imgs shape: \" +str(orig_imgs.shape))\n",
    "print (\"pred imgs shape: \" +str(pred_imgs.shape))\n",
    "print (\"Gtruth imgs shape: \" +str(gtruth_masks.shape))\n",
    "\n",
    "visualize(group_images(orig_imgs,N_visual),path_experiment+\"all_originals_thr\")#.show()\n",
    "visualize(group_images(pred_imgs_thr,N_visual),path_experiment+\"all_predictions_thr\")#.show()\n",
    "visualize(group_images(gtruth_masks,N_visual),path_experiment+\"all_groundTruths_thr\")#.show()\n",
    "#visualize results comparing mask and prediction:\n",
    "assert (orig_imgs.shape[0]==pred_imgs_thr.shape[0] and orig_imgs.shape[0]==gtruth_masks.shape[0])\n",
    "N_predicted = orig_imgs.shape[0]\n",
    "group = N_visual\n",
    "assert (N_predicted%group==0)\n",
    "\n",
    "result_path = 'each_results_thr'\n",
    "if os.path.isdir(path_experiment+result_path) == False:\n",
    "    os.mkdir(path_experiment +result_path)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format(path_experiment +result_path))\n",
    "\n",
    "for i in range(int(N_predicted/group)):\n",
    "    orig_stripe = group_images(orig_imgs[i*group:(i*group)+group,:,:,:],group)\n",
    "    masks_stripe = group_images(gtruth_masks[i*group:(i*group)+group,:,:,:],group)\n",
    "    pred_stripe = group_images(pred_imgs_thr[i*group:(i*group)+group,:,:,:],group)\n",
    "    total_img = np.concatenate((orig_stripe,masks_stripe,pred_stripe),axis=0)\n",
    "    \n",
    "    visualize(pred_stripe,path_experiment+ result_path +'/'+\"_Prediction\"+str(i))#.show()\n",
    "    visualize(total_img,path_experiment+ result_path +'/'+\"_Original_GroundTruth_Prediction\"+str(i))#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores, y_true = pred_only_FOV(pred_imgs_thr, gtruth_masks, test_border_masks)\n",
    "print (\"Calculating results only inside the FOV:\")\n",
    "print (\"y scores pixels: \" +str(y_scores.shape[0]) +\" (radius 270: 270*270*3.14==228906), including background around retina: \" +str(pred_imgs.shape[0]*pred_imgs.shape[2]*pred_imgs.shape[3]) +\" (584*565==329960)\")\n",
    "print (\"y true pixels: \" +str(y_true.shape[0]) +\" (radius 270: 270*270*3.14==228906), including background around retina: \" +str(gtruth_masks.shape[2]*gtruth_masks.shape[3]*gtruth_masks.shape[0])+\" (584*565==329960)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "#roc curve\n",
    "fpr, tpr, thresholds = roc_curve((y_true), y_scores)\n",
    "auc_roc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "print (\"\\nArea under the ROC curve: \" +str(auc_roc))\n",
    "roc_curve = plt.figure()\n",
    "\n",
    "plt.plot(fpr,tpr,'-',label='Area Under the Curve (AUC = %0.4f)' % auc_roc)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel(\"FPR (False Positive Rate)\")\n",
    "plt.ylabel(\"TPR (True Positive Rate)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(path_experiment+\"ROC_thr.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "precision = np.fliplr([precision])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "recall = np.fliplr([recall])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "AUC_prec_rec = np.trapz(precision,recall)\n",
    "print (\"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec))\n",
    "prec_rec_curve = plt.figure()\n",
    "\n",
    "plt.plot(recall,precision,'-',label='Area Under the Curve (AUC = %0.4f)' % AUC_prec_rec)\n",
    "plt.title('Precision - Recall curve')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(path_experiment+\"Precision_recall_thr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "threshold_confusion = 0.5\n",
    "print (\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
    "y_pred = np.empty((y_scores.shape[0]))\n",
    "for i in range(y_scores.shape[0]):\n",
    "    if y_scores[i]>=threshold_confusion:\n",
    "        y_pred[i]=1\n",
    "    else:\n",
    "        y_pred[i]=0\n",
    "        \n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "print (confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "if float(np.sum(confusion))!=0:\n",
    "    accuracy = float(confusion[0,0]+confusion[1,1])/float(np.sum(confusion))\n",
    "print (\"Global Accuracy: \" +str(accuracy))\n",
    "\n",
    "specificity = 0\n",
    "#Jaccard similarity index\n",
    "jaccard_index = jaccard_similarity_score(y_true, y_pred, normalize=True)\n",
    "print (\"\\nJaccard similarity score: \" +str(jaccard_index))\n",
    "\n",
    "#F1 score\n",
    "F1_score = f1_score(y_true, y_pred, labels=None, average='binary', sample_weight=None)\n",
    "print (\"\\nF1 score (F-measure): \" +str(F1_score))\n",
    "\n",
    "#Save the results\n",
    "file_perf = open(path_experiment+'/performances.txt', 'w')\n",
    "file_perf.write(\"Area under the ROC curve: \"+str(auc_roc)\n",
    "                + \"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec)\n",
    "                + \"\\nJaccard similarity score: \" +str(jaccard_index)\n",
    "                + \"\\nF1 score (F-measure): \" +str(F1_score)\n",
    "                +\"\\n\\nConfusion matrix:\"\n",
    "                +str(confusion)\n",
    "                +\"\\nACCURACY: \" +str(accuracy)\n",
    "                +\"\\nSENSITIVITY: \" +str(sensitivity)\n",
    "                +\"\\nSPECIFICITY: \" +str(specificity)\n",
    "                +\"\\nPRECISION: \" +str(precision)\n",
    "                )\n",
    "file_perf.close()\n",
    "if float(confusion[0,0]+confusion[0,1])!=0:\n",
    "    specificity = float(confusion[0,0])/float(confusion[0,0]+confusion[0,1])\n",
    "print (\"Specificity: \" +str(specificity))\n",
    "sensitivity = 0\n",
    "if float(confusion[1,1]+confusion[1,0])!=0:\n",
    "    sensitivity = float(confusion[1,1])/float(confusion[1,1]+confusion[1,0])\n",
    "print (\"Sensitivity: \" +str(sensitivity))\n",
    "precision = 0\n",
    "if float(confusion[1,1]+confusion[0,1])!=0:\n",
    "    precision = float(confusion[1,1])/float(confusion[1,1]+confusion[0,1])\n",
    "print (\"Precision: \" +str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Jaccard similarity index\n",
    "jaccard_index = jaccard_similarity_score(y_true, y_pred, normalize=True)\n",
    "print (\"\\nJaccard similarity score: \" +str(jaccard_index))\n",
    "\n",
    "#F1 score\n",
    "F1_score = f1_score(y_true, y_pred, labels=None, average='binary', sample_weight=None)\n",
    "print (\"\\nF1 score (F-measure): \" +str(F1_score))\n",
    "\n",
    "#Save the results\n",
    "file_perf = open(path_experiment+'/performances_thr.txt', 'w')\n",
    "file_perf.write(\"Area under the ROC curve: \"+str(auc_roc)\n",
    "                + \"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec)\n",
    "                + \"\\nJaccard similarity score: \" +str(jaccard_index)\n",
    "                + \"\\nF1 score (F-measure): \" +str(F1_score)\n",
    "                +\"\\n\\nConfusion matrix:\"\n",
    "                +str(confusion)\n",
    "                +\"\\nACCURACY: \" +str(accuracy)\n",
    "                +\"\\nSENSITIVITY: \" +str(sensitivity)\n",
    "                +\"\\nSPECIFICITY: \" +str(specificity)\n",
    "                +\"\\nPRECISION: \" +str(precision)\n",
    "                )\n",
    "file_perf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
