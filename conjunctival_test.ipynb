{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Python\n",
    "#import module for predict\n",
    "import os\n",
    "import numpy as np\n",
    "import configparser\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "#Keras\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model\n",
    "import keras.backend\n",
    "#scikit learn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "import sys\n",
    "sys.path.insert(0, './lib/')\n",
    "# help_functions.py\n",
    "from help_functions import *\n",
    "# extract_patches.py\n",
    "from extract_patches import *\n",
    "# pre_processing.py\n",
    "from pre_processing import *\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "def show_on_jupyter(img,color= None,title=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cv2\n",
    "    \"\"\"Show img on jupyter notebook. No return Value\n",
    "    \n",
    "    You should check the img's color space.\n",
    "    I just consider about RGB color space & 1 ch color space(like green ch, gray space, ...)\n",
    "    \n",
    "    using matplotlib\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : 2-D Array\n",
    "        numpy 2-D array\n",
    "        opencv / sklearn / plt are avaliable.\n",
    "        float / uint8 data type.\n",
    "        \n",
    "    color : string\n",
    "        'gray' or 'None'\n",
    "        'gray' means that img has a 1 ch.\n",
    "        'None' means that img has a RGB ch.\n",
    "        (default: None)\n",
    "        \n",
    "    title : string\n",
    "        decide img's title\n",
    "        (default : None)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        No return value.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> img = cv2.imread(img_path)\n",
    "    >>> show_on_jupyter(img)\n",
    "    \n",
    "    img has a 1 ch\n",
    "    >>> img = cv2.imread(img_path)\n",
    "    >>> show_on_jupyter(img,'gray')\n",
    "    \"\"\"\n",
    "    if color == 'gray':\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(img,cmap=color)\n",
    "        plt.show()\n",
    "    elif color == None:\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Gray or None\")\n",
    "\n",
    "\n",
    "def load_data(data_path):\n",
    "    \n",
    "    imgs = np.empty((NUM_IMG,1,HEIGHT_IMG,WIDTH_IMG))\n",
    "    cnt = 0\n",
    "    for path, subdirs, files in os.walk(data_path): #list all files, directories in the path\n",
    "        for i in range(len(files)):\n",
    "            tempImg = Image.open(data_path + files[i])\n",
    "            #print(files[i])\n",
    "            #tempImg = np.asarray(tempImg)\n",
    "            #print(tempImg.shape)\n",
    "            w, h = tempImg.size\n",
    "            print(w,h)\n",
    "            print('w,h : ', w,'x',h)\n",
    "            if (w == 700 and h ==380):\n",
    "                print(\"Hi\")\n",
    "                tempImg = np.asarray(tempImg)\n",
    "                tempImg = tempImg[np.newaxis,:,:]\n",
    "                imgs[cnt] = tempImg\n",
    "                cnt += 1\n",
    "            \n",
    "    return imgs\n",
    "\n",
    "def group_plot(data,row,col):\n",
    "    fig=plt.figure(figsize=(13, 13))\n",
    "    columns = col\n",
    "    rows = row\n",
    "    for num in range(len(data)):\n",
    "        img = data[num]\n",
    "        fig.add_subplot(rows, columns, num+1)\n",
    "        plt.imshow(img,cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def get_conjunctivaData_testing(Conjunctiva_test_img_ori, num_test_img,patch_h, patch_w):\n",
    "    \n",
    "    # get img data\n",
    "    test_img_ori = Conjunctiva_test_img_ori\n",
    "    \n",
    "    test_imgs = my_gray_preprocessing(test_img_ori)\n",
    "    \n",
    "    # extend both images and masks so they can be divided exactly by the patches dimensions\n",
    "    # make tensor data.\n",
    "    \n",
    "    test_imgs = test_imgs[0:num_test_img,:,:,:]\n",
    "    print('type : ',type(test_imgs))\n",
    "    print('[get data testing func] prev test img shape : {} '.format(test_imgs.shape))\n",
    "    test_imgs = paint_border(test_imgs,patch_h, patch_w)\n",
    "    print('[get data testing func] after test img shape : {} '.format(test_imgs.shape))\n",
    "\n",
    "    #data_consistency_check(test_imgs, test_grds)\n",
    "    #assert(np.max(test_grds)==1  and np.min(test_grds)==0)\n",
    "    \n",
    "    print (\"[get_data_testing_func] test images range (min-max): \" +str(np.min(test_imgs)) +' - '+str(np.max(test_imgs)))\n",
    "    print (\"[get_data_testing_fucn] test masks are within 0-1\\n\")\n",
    "    \n",
    "def width_padding(img, pad_size):\n",
    "    h,w = np.shape(img)\n",
    "    #print('pad size', pad_size)\n",
    "    \n",
    "    if pad_size % 2 ==1: #odd number\n",
    "        pad01 = np.zeros((h, pad_size//2))\n",
    "        pad02 = np.zeros((h, pad_size//2 +1))\n",
    "    \n",
    "        result = np.hstack([pad01, img])\n",
    "        result = np.hstack([result, pad02])\n",
    "        \n",
    "    else:\n",
    "        pad01 = np.zeros((h, pad_size//2))\n",
    "    \n",
    "        result = np.hstack([pad01, img])\n",
    "        result = np.hstack([result, pad01])\n",
    "        \n",
    "    print(np.shape(img), ' ',np.shape(result))\n",
    "    return result\n",
    "    \n",
    "def height_padding(img,pad_size):\n",
    "    h,w = np.shape(img)\n",
    "    #print('pad size : ',pad_size)\n",
    "    \n",
    "    if pad_size % 2 ==1: #odd number\n",
    "        pad01 = np.zeros((pad_size//2,w ))\n",
    "        pad02 = np.zeros((pad_size//2 +1 , w))\n",
    "        \n",
    "        result = np.vstack([pad01, img])\n",
    "        result = np.vstack([result,pad02])\n",
    "        \n",
    "    else:\n",
    "        pad01 = np.zeros((pad_size//2, w))\n",
    "    \n",
    "        result = np.vstack([pad01, img])\n",
    "        result = np.vstack([result, pad01])\n",
    "    print(np.shape(img), ' ',np.shape(result))\n",
    "    return result\n",
    "\n",
    "    patches_imgs_test = extract_ordered(test_imgs,patch_h,patch_w)\n",
    "    #data_consistency_check(test_imgs, test_grds)\n",
    "\n",
    "    print (\"\\n[get_data_testing_fucn] test PATCHES images/grds shape:\")\n",
    "    print (patches_imgs_test.shape)\n",
    "    print (\"[get_data_testing_fucn] test PATCHES images range (min-max): {} - {}\".format(str(np.min(patches_imgs_test)), str(np.max(patches_imgs_test))))\n",
    "\n",
    "    return patches_imgs_test\n",
    "\n",
    "def get_conjunctivaData_testing_overlap(Conjunctiva_test_img_ori, num_test_img,\n",
    "                                        patch_h,patch_w,stride_h, stride_w):\n",
    "    \n",
    "    # get img data\n",
    "    test_img_ori = Conjunctiva_test_img_ori\n",
    "    print(np.shape(test_img_ori))\n",
    "    test_imgs = my_gray_preprocessing(test_img_ori)\n",
    "    \n",
    "    # extend both images and masks so they can be divided exactly by the patches dimensions\n",
    "    # make tensor data.\n",
    "    \n",
    "    test_imgs = test_imgs[0:num_test_img,:,:,:]\n",
    "    \n",
    "    #print('type : ',type(test_imgs))\n",
    "    #print('[get data testing overlap] prev test img shape : {}  '.format(test_imgs.shape))\n",
    "    test_imgs = paint_border_overlap(test_imgs,patch_h, patch_w,stride_h, stride_w)\n",
    "    #test_grds = paint_border_overlap(test_grds, patch_h, patch_w,stride_h, stride_w)\n",
    "    #print('[get data testing overlap] after test img shape : {}'.format(test_imgs.shape))\n",
    "    \n",
    "    #print (\"[get_data_testing_overlap func] test images range (min-max): \" +str(np.min(test_imgs)) +' - '+str(np.max(test_imgs)))\n",
    "    #print (\"[get_data_testing_overlap fucn] test masks are within 0-1\\n\")\n",
    "    \n",
    "    patches_imgs_test = extract_ordered_overlap(test_imgs,patch_h,patch_w,stride_h,stride_w)\n",
    "    \n",
    "    #print (\"\\n[get_data_testing_overlap func] test PATCHES images shape:\")\n",
    "    print (patches_imgs_test.shape)\n",
    "    #print (\"[get_data_testing_overlap func] test PATCHES images range (min-max): \" +str(np.min(patches_imgs_test)) +' - '+str(np.max(patches_imgs_test)))\n",
    "\n",
    "    return patches_imgs_test, test_imgs.shape[2], test_imgs.shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./conjunctival_test_norm_150000\n",
      "./conj_train/\n",
      "DRIVE_test_norm_150000 \n",
      " ./DRIVE_test_norm_150000/\n",
      "conjunctival_test_norm_150000\n",
      "./conjunctival_test_norm_150000/\n"
     ]
    }
   ],
   "source": [
    "#========= CONFIG FILE TO READ FROM =======\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('conj_configuration.txt')\n",
    "\n",
    "#run the training on invariant or local\n",
    "path_data = config.get('data paths', 'conj_path_local')\n",
    "\n",
    "patch_height = int(config.get('data attributes', 'patch_height'))\n",
    "patch_width = int(config.get('data attributes', 'patch_width'))\n",
    "\n",
    "stride_height = int(config.get('testing settings', 'stride_height'))\n",
    "stride_width = int(config.get('testing settings', 'stride_width'))\n",
    "\n",
    "name_experiment = config.get('experiment name', 'name')\n",
    "path_experiment = './' +name_experiment +'/'\n",
    "average_mode = config.getboolean('testing settings', 'average_mode')\n",
    "\n",
    "target_experiment = config.get('testing settings','experiment_target')\n",
    "target_path = './' + target_experiment + '/'\n",
    "\n",
    "if os.path.isdir('./'+name_experiment) == False:\n",
    "    os.mkdir('./'+name_experiment)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format('./'+name_experiment))\n",
    "\n",
    "print(path_data)\n",
    "print(target_experiment, '\\n', target_path)\n",
    "print(name_experiment)\n",
    "print(path_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./conj_train/\n",
      "./conj_train/train_data_24.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_5.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_34.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_1.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_17.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_11.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_8.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_14.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_4.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_19.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_25.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_13.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_22.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_7.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_18.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_15.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_32.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_28.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_31.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_9.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_21.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_12.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_3.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_16.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_27.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_20.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_33.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_10.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_29.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_2.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_23.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_26.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_30.png\n",
      "(584, 565)\n",
      "./conj_train/train_data_6.png\n",
      "(584, 565)\n",
      "(34, 584, 565)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "HEIGHT_IMG = 584\n",
    "WIDTH_IMG = 565\n",
    "NUM_IMG = 0\n",
    "CONJ_IMG_LIST = []\n",
    "\n",
    "print(path_data)\n",
    "\n",
    "for path, subdirs, files in os.walk(path_data): #list all files, directories in the path\n",
    "        for i in range(len(files)):\n",
    "            print(path_data+files[i])\n",
    "            tempImg = cv2.imread(path_data + files[i],0)\n",
    "            print(np.shape(tempImg))\n",
    "            CONJ_IMG_LIST.append(tempImg)\n",
    "        \n",
    "            NUM_IMG = NUM_IMG +1\n",
    "            \n",
    "CONJ_IMG_LIST = np.asarray(CONJ_IMG_LIST)\n",
    "print(np.shape(CONJ_IMG_LIST))\n",
    "print(type(CONJ_IMG_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 1, 584, 565)\n",
      "<class 'numpy.ndarray'>\n",
      "(34, 1, 584, 565)\n"
     ]
    }
   ],
   "source": [
    "temp = np.expand_dims(CONJ_IMG_LIST,axis =1)\n",
    "print(np.shape(temp))\n",
    "temp = np.asarray(temp)\n",
    "print(type(temp))\n",
    "print(np.shape(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 1, 584, 565)\n",
      "\n",
      "the side H is not compatible with the selected stride of 5\n",
      "img_h 584, patch_h 48, stride_h 5\n",
      "(img_h - patch_h) MOD stride_h: 1\n",
      "So the H dim will be padded with additional 4 pixels\n",
      "\n",
      "the side H is not compatible with the selected stride of 5\n",
      "img_h 565, patch_h 48, stride_h 5\n",
      "(img_h - patch_h) MOD stride_h: 2\n",
      "So the H dim will be padded with additional 3 pixels\n",
      "new full images shape: \n",
      "(34, 1, 588, 568)\n",
      "extract , :  540 5 520 5\n",
      "[extrct_order_overlap func] Number of patches on h : 109\n",
      "[extrct_order_overlap func] Number of patches on w : 105\n",
      "[extrct_order_overlap func] number of patches per image: 11445 totally for this dataset: 389130\n",
      "(389130, 1, 48, 48)\n",
      "[group images func] prev data shape  : (100, 1, 48, 48)\n",
      "[group images func] after data shape :  (100, 48, 48, 1)\n",
      "[group images func] first total image :  (48, 96, 1)\n",
      "[group images func] final total image :  (2448, 96, 1)\n",
      "data shape :  (2448, 96, 1)\n",
      "<PIL.Image.Image image mode=L size=96x2448 at 0x7F74BE962C88>\n",
      "file name :  ./conjunctival_test_norm_150000/conj_overlap_test_patch_img\n"
     ]
    }
   ],
   "source": [
    "#conj_img = load_data(path_data) \n",
    "conj_img = np.expand_dims(CONJ_IMG_LIST,axis =1)\n",
    "\n",
    "if average_mode == True:\n",
    "    patches_conj_imgs_test, new_height, new_width = get_conjunctivaData_testing_overlap(conj_img,\n",
    "                                                         NUM_IMG,\n",
    "                                                         patch_height,\n",
    "                                                         patch_width,\n",
    "                                                         stride_height,\n",
    "                                                        stride_width)\n",
    "    conj_totimg = visualize(group_images(patches_conj_imgs_test[100:200,:,:,:],2),path_experiment+'conj_overlap_test_patch_img')\n",
    "else:\n",
    "    patches_conj_imgs_test = get_conjunctivaData_testing(conj_img,\n",
    "                                                         NUM_IMG,\n",
    "                                                         patch_height,\n",
    "                                                         patch_width)\n",
    "    \n",
    "    conj_totimg = visualize(group_images(patches_conj_imgs_test[100:200,:,:,:],2),path_experiment+'conj_test_patch_img')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ Run the prediction of the patches ==================================\n",
    "best_last = config.get('testing settings', 'best_last')\n",
    "#Load the saved model\n",
    "model = model_from_json(open(target_path +target_experiment+'_architecture.json').read())\n",
    "model.load_weights(target_path+best_last+'_weights.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DRIVE_test_norm_150000/DRIVE_test_norm_150000_architecture.json\n",
      "./DRIVE_test_norm_150000/ \n",
      " best     _\n",
      "<keras.engine.training.Model object at 0x7f74bdda6438>\n"
     ]
    }
   ],
   "source": [
    "print(target_path +target_experiment+'_architecture.json')\n",
    "print(target_path,'\\n', best_last, '    _')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389130, 1, 48, 48)\n",
      "(50000, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(patches_conj_imgs_test))\n",
    "\n",
    "temp_conj_patchs = patches_conj_imgs_test[: 50000,:,:,:]\n",
    "print(np.shape(temp_conj_patchs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conj_predictions = model.predict(patches_conj_imgs_test, batch_size=32, verbose=1)\n",
    "\n",
    "conj_predictions = model.predict(temp_conj_patchs, batch_size=128, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[group images func] prev data shape :  (50, 1, 48, 48)\n",
      "[group images func] after data shape :  (50, 48, 48, 1)\n",
      "[group images func] first total image :  (48, 96, 1)\n",
      "[group images func] final total image :  (1248, 96, 1)\n",
      "data shape :  (1248, 96, 1)\n",
      "<PIL.Image.Image image mode=L size=96x1248 at 0x7F5DA65334E0>\n",
      "file name :  ./conjunctival_test_norm_150000//predict_conj_patch_ori\n",
      "[group images func] prev data shape :  (50, 1, 48, 48)\n",
      "[group images func] after data shape :  (50, 48, 48, 1)\n",
      "[group images func] first total image :  (48, 96, 1)\n",
      "[group images func] final total image :  (1248, 96, 1)\n",
      "data shape :  (1248, 96, 1)\n",
      "<PIL.Image.Image image mode=L size=96x1248 at 0x7F5DA6533AC8>\n",
      "file name :  ./conjunctival_test_norm_150000//predict_conj_patch_thresh\n"
     ]
    }
   ],
   "source": [
    "pred_conj_patches = pred_to_imgs(conj_predictions, patch_height, patch_width, \"original\")\n",
    "pred_conj_patches_thresh = pred_to_imgs(conj_predictions, patch_height, patch_width, \"threshold\")\n",
    "\n",
    "\n",
    "temp5_totimg = visualize(group_images(pred_conj_patches[150:200,:,:,:],2),path_experiment+'/predict_conj_patch_ori')\n",
    "temp6_totimg = visualize(group_images(pred_conj_patches_thresh[150:200,:,:,:],2),path_experiment+'/predict_conj_patch_thresh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_patches_h: 68\n",
      "N_patches_w: 132\n",
      "N_patches_img: 8976\n",
      "According to the dimension inserted, there are 2 full images (of 383x703 each)\n",
      "(2, 1, 383, 703)\n",
      "N_patches_h: 68\n",
      "N_patches_w: 132\n",
      "N_patches_img: 8976\n",
      "According to the dimension inserted, there are 2 full images (of 383x703 each)\n",
      "(2, 1, 383, 703)\n",
      "N_patches_h: 68\n",
      "N_patches_w: 132\n",
      "N_patches_img: 8976\n",
      "According to the dimension inserted, there are 2 full images (of 383x703 each)\n",
      "(2, 1, 383, 703)\n",
      "[group images func] prev data shape :  (2, 1, 383, 703)\n",
      "[group images func] after data shape :  (2, 383, 703, 1)\n",
      "[group images func] first total image :  (383, 1406, 1)\n",
      "[group images func] final total image :  (766, 1406, 1)\n",
      "data shape :  (766, 1406, 1)\n",
      "<PIL.Image.Image image mode=L size=1406x766 at 0x7F5DA64D6BA8>\n",
      "file name :  ./conjunctival_test_norm_150000//conj_predict_imgs\n",
      "[group images func] prev data shape :  (2, 1, 383, 703)\n",
      "[group images func] after data shape :  (2, 383, 703, 1)\n",
      "[group images func] first total image :  (383, 1406, 1)\n",
      "[group images func] final total image :  (766, 1406, 1)\n",
      "data shape :  (766, 1406, 1)\n",
      "<PIL.Image.Image image mode=L size=1406x766 at 0x7F5DA64D6B00>\n",
      "file name :  ./conjunctival_test_norm_150000//conj_predict_thresh_imgs\n",
      "[group images func] prev data shape :  (2, 1, 383, 703)\n",
      "[group images func] after data shape :  (2, 383, 703, 1)\n",
      "[group images func] first total image :  (383, 1406, 1)\n",
      "[group images func] final total image :  (766, 1406, 1)\n",
      "data shape :  (766, 1406, 1)\n",
      "<PIL.Image.Image image mode=L size=1406x766 at 0x7F5DA74009E8>\n",
      "file name :  ./conjunctival_test_norm_150000//conj_ori_imgs\n"
     ]
    }
   ],
   "source": [
    "if average_mode == True:\n",
    "    conj_pred_imgs = recompose_overlap_img(pred_conj_patches, new_height, new_width, stride_height, stride_width)# predictions\n",
    "    conj_pred_imgs_thr = recompose_overlap_img(pred_conj_patches_thresh, new_height, new_width, stride_height, stride_width)# predictions\n",
    "    conj_ori_imgs = recompose_overlap_img(patches_conj_imgs_test,new_height, new_width, stride_height, stride_width)\n",
    "   \n",
    "    tempConj01_totimg = visualize(group_images(conj_pred_imgs[:,:,:,:],2),path_experiment+'/conj_predict_imgs')\n",
    "    tempConj02_totimg = visualize(group_images(conj_pred_imgs_thr[:,:,:,:],2),path_experiment+'/conj_predict_thresh_imgs')\n",
    "    tempConj03_totimg = visualize(group_images(conj_ori_imgs[:,:,:,:],2), path_experiment+'/conj_ori_imgs')\n",
    "\n",
    "\n",
    "else:\n",
    "    conj_pred_imgs = recompose_img(pred_conj_patches,num_patch_per_img_h,num_patch_per_img_w)\n",
    "    conj_pred_imgs_thresh = recompose_img(pred_conj_patches_thresh,num_patch_per_img_h,num_patch_per_img_w)\n",
    "    conj_ori_imgs = recompose_img(patches_conj_imgs_test,num_patch_per_img_h,num_patch_per_img_w)\n",
    "\n",
    "    tempConj01_totimg = visualize(group_images(conj_pred_imgs[:,:,:,:],2),path_experiment+'/conj_predict_imgs')\n",
    "    tempConj02_totimg = visualize(group_images(conj_pred_imgs_thresh[:,:,:,:],2),path_experiment+'/conj_predict_thresh_imgs')\n",
    "    tempConj03_totimg = visualize(group_images(conj_ori_imgs[:,:,:,:],2), path_experiment+'/conj_ori_imgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
