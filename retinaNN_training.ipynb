{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model as plot\n",
    "from keras.optimizers import SGD\n",
    "from keras import models\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './lib/')\n",
    "from help_functions import *\n",
    "\n",
    "#function to obtain data for training/testing (validation)\n",
    "from extract_patches import get_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom U-net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(n_ch,patch_height,patch_width):\n",
    "    K.set_image_data_format('channels_first')\n",
    "    inputs = layers.Input(shape=(n_ch,patch_height,patch_width))\n",
    "    conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Dropout(0.2)(conv1)\n",
    "    conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "    #\n",
    "    conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Dropout(0.2)(conv2)\n",
    "    conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "    #\n",
    "    conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Dropout(0.2)(conv3)\n",
    "    conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "\n",
    "    up1 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
    "    up1 = layers.concatenate([conv2,up1],axis=1)\n",
    "    conv4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up1)\n",
    "    conv4 = layers.Dropout(0.2)(conv4)\n",
    "    conv4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    #\n",
    "    up2 = layers.UpSampling2D(size=(2, 2),data_format='channels_first')(conv4)\n",
    "    up2 = layers.concatenate([conv1,up2], axis=1)\n",
    "    conv5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
    "    conv5 = layers.Dropout(0.2)(conv5)\n",
    "    conv5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    #\n",
    "    conv6 = layers.Conv2D(2, (1, 1), activation='relu',padding='same')(conv5)\n",
    "    conv6 = layers.core.Reshape((2,patch_height*patch_width))(conv6)\n",
    "    conv6 = layers.core.Permute((2,1))(conv6)\n",
    "    ############\n",
    "    conv7 = layers.core.Activation('softmax')(conv6)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=conv7)\n",
    "\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch normalization\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    x = layers.Conv2D(filters=n_filters, kernel_size = (kernel_size, kernel_size), kernel_initializer= 'he_normal',\n",
    "                     padding = 'same')(input_tensor)\n",
    "    \n",
    "    if batchnorm ==True:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters=n_filters, kernel_size = (kernel_size, kernel_size), kernel_initializer= 'he_normal',\n",
    "                     padding = 'same')(x)\n",
    "    if batchnorm ==True:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For Attention block\n",
    "\n",
    "[To Do]\n",
    "    - Explain 'Function what to do'\n",
    "    - Explain 'What is the self-attention in cnn?'\n",
    "    - Explain 'Novelty of this system'\n",
    "\n",
    "'''\n",
    "\n",
    "def expend_as(tensor, rep, name):\n",
    "    my_repeat = layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis = 3), arguments={'repnum' : rep}, name = 'psi_up'+name)(tensor)\n",
    "    return my_repeat\n",
    "\n",
    "def AttnGatingBlock(x, g, inter_shape, name):\n",
    "    ''' take g which is the spatially smaller signal, do a conv to get the same\n",
    "    number of feature channels as x (bigger spatially)\n",
    "    do a conv on x to also get same geature channels (theta_x)\n",
    "    then, upsample g to be same size as x \n",
    "    add x and g (concat_xg)\n",
    "    relu, 1x1 conv, then sigmoid then upsample the final - this gives us attn coefficients'''\n",
    "    \n",
    "    shape_x = K.int_shape(x)\n",
    "    shape_g = K.int_shape(g)\n",
    "    \n",
    "    print('shape x,g ', shape_x,shape_g)\n",
    "    \n",
    "    theta_x = layers.Conv2D(inter_shape, (2,2), strides=(2,2), padding='same', name = 'xl'+name)(x)\n",
    "    shape_theta_x  = K.int_shape(theta_x)\n",
    "    print('theha x,shape theta_x ', np.shape(theta_x), shape_theta_x)\n",
    "\n",
    "    \n",
    "    phi_g = layers.Conv2D(inter_shape, (1,1), padding='same')(g)\n",
    "    \n",
    "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same', name='g_up'+name)(phi_g)  # 16\n",
    "\n",
    "    print('upsample_g ', np.shape(upsample_g))\n",
    "    \n",
    "    concat_xg = layers.merge.add([upsample_g, theta_x])\n",
    "    act_xg = layers.Activation('relu')(concat_xg)\n",
    "    \n",
    "    psi = layers.Conv2D(1, (1,1), padding= 'same', name = 'psi'+name)(act_xg)\n",
    "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)\n",
    "    \n",
    "    upsample_psi = expend_as(upsample_psi, shape_x[3], name)\n",
    "    \n",
    "    y = layers.merge.multiply([upsample_psi, x], name = 'q_attn'+name)\n",
    "    \n",
    "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same',name='q_attn_conv'+name)(y)\n",
    "    result_bn = layers.BatchNormalization(name='q_attn_bn'+name)(result)\n",
    "    \n",
    "    return result_bn\n",
    "\n",
    "def UnetGatingSignal(input, is_batchnorm, name):\n",
    "    '''\n",
    "    this is simply 1x1 convolution, bn, activation\n",
    "    '''\n",
    "    \n",
    "    shape = K.int_shape(input)\n",
    "    x = layers.Conv2D(shape[3] * 1, (1, 1), strides=(1, 1), padding=\"same\", name=name + '_conv')(input)\n",
    "    if is_batchnorm:\n",
    "        x = layers.BatchNormalization(name=name + '_bn')(x)\n",
    "    x = layers.Activation('relu', name = name+ '_act')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(input_feature, ratio=8):\n",
    "    print('input feature shape', input_feature._keras_shape)\n",
    "    channel = input_feature._keras_shape[1]\n",
    "\n",
    "    shared_layer_one = layers.Dense(channel // ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    shared_layer_two = layers.Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "\n",
    "    avg_pool = layers.GlobalAveragePooling2D()(input_feature)\n",
    "    print('before reshpae avg pool', avg_pool._keras_shape)\n",
    "    avg_pool = layers.Reshape((channel,1, 1))(avg_pool)\n",
    "    print('after reshpae avg pool', avg_pool._keras_shape)\n",
    "\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    print('after shared layer 01 avg pool', avg_pool._keras_shape)\n",
    "\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    print('after shared layer 02 avg pool', avg_pool._keras_shape)\n",
    "\n",
    "\n",
    "    max_pool = layers.GlobalMaxPooling2D()(input_feature)\n",
    "    print('before reshpae max pool', max_pool._keras_shape)\n",
    "\n",
    "    max_pool = layers.Reshape((channel,1, 1))(max_pool)\n",
    "    print('after reshpae max pool', max_pool._keras_shape)\n",
    "\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    print('after shared layer 01 max pool', max_pool._keras_shape)\n",
    "\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    print('before shared layer 02 max pool', max_pool._keras_shape)\n",
    "\n",
    "\n",
    "    cbam_feature = layers.Add()([avg_pool, max_pool])\n",
    "    cbam_feature = layers.Activation('sigmoid')(cbam_feature)\n",
    "    \n",
    "    return layers.multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature, kernel_size=7):\n",
    "        avg_pool = layers.Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(input_feature)\n",
    "        #print('shape of avg pool and input feature : ', np.shape(input_feature), np.shape(avg_pool))\n",
    "        max_pool = layers.Lambda(lambda x: K.max(x, axis=1, keepdims=True))(input_feature)\n",
    "        #print('shape of max pool and input feature : ', np.shape(input_feature), np.shape(max_pool))\n",
    "\n",
    "        concat = layers.Concatenate(axis=1)([avg_pool, max_pool])\n",
    "        \n",
    "        cbam_feature = layers.Conv2D(filters=1,\n",
    "                      kernel_size=kernel_size,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='sigmoid',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      use_bias=False)(concat)\n",
    "        return layers.multiply([input_feature, cbam_feature])\n",
    "    \n",
    "def cbam_block(cbam_feature, ratio=2):\n",
    "        # https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n",
    "        cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "        cbam_feature = spatial_attention(cbam_feature)\n",
    "        return cbam_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_norm(n_ch,patch_height,patch_width):\n",
    "    K.set_image_data_format('channels_first')\n",
    "    inputs = layers.Input(shape=(n_ch,patch_height,patch_width))\n",
    "    conv1 = conv2d_block(inputs, n_filters= 32, kernel_size=3, batchnorm=True)\n",
    "    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "    #\n",
    "    conv2 = conv2d_block(pool1, n_filters= 64, kernel_size=3, batchnorm=True)\n",
    "    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "    #\n",
    "    conv3 = conv2d_block(pool2, n_filters= 128, kernel_size=3, batchnorm=True)\n",
    "    up1 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
    "    up1 = layers.concatenate([conv2,up1],axis=1)\n",
    "    \n",
    "    \n",
    "    conv4 = conv2d_block(up1, n_filters= 64, kernel_size=3, batchnorm=True)\n",
    "    \n",
    "    \n",
    "    up2 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
    "    up2 = layers.concatenate([conv1,up2], axis=1)\n",
    "    conv5 = conv2d_block(up2, n_filters= 32, kernel_size=3, batchnorm=True)\n",
    "    \n",
    "    conv6 = layers.Conv2D(2, (1, 1), activation='relu',padding='same')(conv5)\n",
    "    \n",
    "    conv6 = layers.core.Reshape((2,patch_height*patch_width))(conv6)\n",
    "    conv6 = layers.core.Permute((2,1))(conv6)\n",
    "    ############\n",
    "    conv7 = layers.core.Activation('softmax')(conv6)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=conv7)\n",
    "\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_attn_unet(n_ch,patch_height,patch_width):\n",
    "    K.set_image_data_format('channels_first')\n",
    "    inputs = layers.Input(shape=(n_ch,patch_height,patch_width))\n",
    "    conv1 = conv2d_block(inputs, n_filters= 32, kernel_size=3, batchnorm=True)\n",
    "    conv1 = layers.SpatialDropout2D(0.1)(conv1)\n",
    "    conv1 = spatial_attention(conv1)\n",
    "    \n",
    "    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "    \n",
    "    #\n",
    "    conv2 = conv2d_block(pool1, n_filters= 64, kernel_size=3, batchnorm=True)\n",
    "    conv2 = layers.SpatialDropout2D(0.1)(conv2)\n",
    "    conv2 = spatial_attention(conv2)\n",
    "    \n",
    "    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "    #center\n",
    "    conv3 = conv2d_block(pool2, n_filters= 128, kernel_size=3, batchnorm=True)\n",
    "    conv3 = layers.SpatialDropout2D(0.4)(conv3)\n",
    "    conv3 = spatial_attention(conv3)\n",
    "    \n",
    "    up1 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
    "    up1 = layers.concatenate([conv2,up1],axis=1)\n",
    "    conv4 = conv2d_block(up1, n_filters= 64, kernel_size=3, batchnorm=True)\n",
    "    conv4 = spatial_attention(conv4)\n",
    "    \n",
    "    up2 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
    "    up2 = layers.concatenate([conv1,up2], axis=1)\n",
    "    conv5 = conv2d_block(up2, n_filters= 32, kernel_size=3, batchnorm=True)\n",
    "    conv5 = spatial_attention(conv5)\n",
    "    \n",
    "    conv6 = layers.Conv2D(2, (1, 1), activation='relu',padding='same')(conv5)\n",
    "    \n",
    "    conv6 = layers.core.Reshape((2,patch_height*patch_width))(conv6)\n",
    "    conv6 = layers.core.Permute((2,1))(conv6)\n",
    "    ############\n",
    "    conv7 = layers.core.Activation('softmax')(conv6)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=conv7)\n",
    "\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=[dice_coef])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam_attn_unet(n_ch,patch_height,patch_width):\n",
    "    K.set_image_data_format('channels_first')\n",
    "    inputs = layers.Input(shape=(n_ch,patch_height,patch_width))\n",
    "    conv1 = conv2d_block(inputs, n_filters= 32, kernel_size=3, batchnorm=True)\n",
    "    conv1 = layers.SpatialDropout2D(0.1)(conv1)\n",
    "    conv1 = cbam_block(conv1)\n",
    "    \n",
    "    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "    \n",
    "    #\n",
    "    conv2 = conv2d_block(pool1, n_filters= 64, kernel_size=3, batchnorm=True)\n",
    "    conv2 = layers.SpatialDropout2D(0.1)(conv2)\n",
    "    conv2 = cbam_block(conv2)\n",
    "    \n",
    "    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "    #center\n",
    "    conv3 = conv2d_block(pool2, n_filters= 128, kernel_size=3, batchnorm=True)\n",
    "    conv3 = layers.SpatialDropout2D(0.4)(conv3)\n",
    "    conv3 = cbam_block(conv3)\n",
    "    \n",
    "    up1 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
    "    up1 = layers.concatenate([conv2,up1],axis=1)\n",
    "    conv4 = conv2d_block(up1, n_filters= 64, kernel_size=3, batchnorm=True)\n",
    "    conv4 = cbam_block(conv4)\n",
    "    \n",
    "    up2 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
    "    up2 = layers.concatenate([conv1,up2], axis=1)\n",
    "    conv5 = conv2d_block(up2, n_filters= 32, kernel_size=3, batchnorm=True)\n",
    "    conv5 = cbam_block(conv5)\n",
    "    \n",
    "    conv6 = layers.Conv2D(2, (1, 1), activation='relu',padding='same')(conv5)\n",
    "    \n",
    "    conv6 = layers.core.Reshape((2,patch_height*patch_width))(conv6)\n",
    "    conv6 = layers.core.Permute((2,1))(conv6)\n",
    "    ############\n",
    "    conv7 = layers.core.Activation('softmax')(conv6)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=conv7)\n",
    "\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_unet(n_ch,patch_height,patch_width):\n",
    "    K.set_image_data_format('channels_first')\n",
    "    inputs = layers.Input(shape=(n_ch,patch_height,patch_width))\n",
    "    conv1 = conv2d_block(inputs, n_filters= 32, kernel_size=3, batchnorm=True)\n",
    "    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "    \n",
    "    conv2 = conv2d_block(pool1, n_filters= 64, kernel_size=3, batchnorm=True)\n",
    "    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "    #center\n",
    "    \n",
    "    conv3 = conv2d_block(pool2, n_filters= 128, kernel_size=3, batchnorm=True)\n",
    "\n",
    "    \n",
    "    gating1 = UnetGatingSignal(conv3,True, 'gating01')\n",
    "    \n",
    "    attn1 = AttnGatingBlock(conv2, gating1, 128 ,'attn01')\n",
    "    up1 = concatenate([Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same',activation=\"relu\")(conv3), attn1], axis=3)\n",
    "    \n",
    "    gating2 = UnetGatingSignal(up1, True, 'gating02')\n",
    "    attn2 = AttnGatingBlock(conv1, gating2, 64,'attn02' )\n",
    "    up2 = concatenate([Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same',activation=\"relu\")(up1), attn2], axis=3)\n",
    "    \n",
    "    \n",
    "    conv6 = layers.Conv2D(2, (1, 1), activation='relu',padding='same')(up2)\n",
    "    \n",
    "    conv6 = layers.core.Reshape((2,patch_height*patch_width))(conv6)\n",
    "    conv6 = layers.core.Permute((2,1))(conv6)\n",
    "    ############\n",
    "    conv7 = layers.core.Activation('softmax')(conv6)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=conv7)\n",
    "\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.RawConfigParser()\n",
    "config.read('configuration.txt')\n",
    "#patch to the datasets\n",
    "path_data = config.get('data paths', 'path_local')\n",
    "#Experiment name\n",
    "save_folder = config.get('experiment name','result_save_path')\n",
    "name_experiment = config.get('experiment name', 'name')\n",
    "\n",
    "#training settings\n",
    "num_epochs = int(config.get('training settings', 'num_epochs'))\n",
    "batch_size = int(config.get('training settings', 'batch_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRIVE_naive_attn_norm02_150000\n",
      "Retina_Result\n"
     ]
    }
   ],
   "source": [
    "print(name_experiment)\n",
    "print(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./Retina_Result/DRIVE_naive_attn_norm02_150000\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('./'+save_folder+'/'+name_experiment) == False:\n",
    "    os.mkdir('./'+save_folder+'/'+name_experiment)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format('./'+save_folder+'/'+name_experiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract patch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : Retina_Result/DRIVE_naive_attn_norm02_150000\n",
      "number of subimages :  150000\n",
      "[DEBUG] shape of train_imgs_original :  (20, 3, 584, 565)\n",
      "[group images func] prev data shape  : (20, 3, 584, 565)\n",
      "[group images func] after data shape :  (20, 584, 565, 3)\n",
      "[group images func] first total image :  (584, 2825, 3)\n",
      "[group images func] final total image :  (2920, 2825, 3)\n",
      "data shape :  (2920, 2825, 3)\n",
      "<PIL.Image.Image image mode=RGB size=2825x2920 at 0x7F919B672B70>\n",
      "file name :  ./Retina_Result/DRIVE_naive_attn_norm02_150000/imgs_train\n",
      "[DEBUG] normalize shape :  (20, 1, 584, 565)\n",
      "[DEBUG] i normalize shape :  (1, 584, 565)\n",
      "[group images func] prev data shape  : (20, 1, 584, 565)\n",
      "[group images func] after data shape :  (20, 584, 565, 1)\n",
      "[group images func] first total image :  (584, 2825, 1)\n",
      "[group images func] final total image :  (2920, 2825, 1)\n",
      "data shape :  (2920, 2825, 1)\n",
      "<PIL.Image.Image image mode=L size=2825x2920 at 0x7F918ECFDDD8>\n",
      "file name :  ./Retina_Result/DRIVE_naive_attn_norm02_150000/preprocessed\n",
      "[get_data_training] preprocessed image shape :  (20, 1, 584, 565)\n",
      "[get_data_training] preprocessed2 image shape :  (20, 1, 565, 565)\n",
      "\n",
      "[get_data_training] train images/masks shape : (20, 1, 565, 565)\n",
      "[get_data_training] train images range (min-max) [0.0 , 1.0] \n",
      "[get_data_training] train masks are within 0-1\n",
      "\n",
      "[extract random] patches shape : (150000, 1, 48, 48)\n",
      "[extract random] patches per full image : 7500\n",
      "\n",
      "[get_data_training] train PATCHES images/masks shape : (150000, 1, 48, 48)\n",
      "[get_data_training] train PATCHES images range (min-max): 0.0 - 1.0\n",
      "[get_data_training] patches_imgs_train : (150000, 1, 48, 48)\n",
      "[group images func] prev data shape  : (50, 1, 48, 48)\n",
      "[group images func] after data shape :  (50, 48, 48, 1)\n",
      "[group images func] first total image :  (48, 240, 1)\n",
      "[group images func] final total image :  (528, 240, 1)\n",
      "data shape :  (528, 240, 1)\n",
      "<PIL.Image.Image image mode=L size=240x528 at 0x7F918EC8E3C8>\n",
      "file name :  ./Retina_Result/DRIVE_naive_attn_norm02_150000/train_patch_img\n"
     ]
    }
   ],
   "source": [
    "patches_imgs_train, patches_masks_train = get_data_training(\n",
    "    DRIVE_train_imgs_original = path_data + config.get('data paths', 'train_imgs_original'),\n",
    "    DRIVE_train_groudTruth = path_data + config.get('data paths', 'train_groundTruth'),  #masks\n",
    "    patch_height = int(config.get('data attributes', 'patch_height')),\n",
    "    patch_width = int(config.get('data attributes', 'patch_width')),\n",
    "    num_subimgs = int(config.get('training settings', 'num_subimgs')),\n",
    "    inside_FOV = config.getboolean('training settings', 'inside_FOV'), #select the patches only inside the FOV  (default == True)\n",
    "    save_path = save_folder+'/'+name_experiment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[group images func] prev data shape  : (40, 1, 48, 48)\n",
      "[group images func] after data shape :  (40, 48, 48, 1)\n",
      "[group images func] first total image :  (48, 240, 1)\n",
      "[group images func] final total image :  (432, 240, 1)\n",
      "data shape :  (432, 240, 1)\n",
      "<PIL.Image.Image image mode=L size=240x432 at 0x7F918EC8E588>\n",
      "file name :  ./Retina_Result/DRIVE_naive_attn_norm02_150000/sample_input_imgs\n",
      "[group images func] prev data shape  : (40, 1, 48, 48)\n",
      "[group images func] after data shape :  (40, 48, 48, 1)\n",
      "[group images func] first total image :  (48, 240, 1)\n",
      "[group images func] final total image :  (432, 240, 1)\n",
      "data shape :  (432, 240, 1)\n",
      "<PIL.Image.Image image mode=L size=240x432 at 0x7F918EC8E668>\n",
      "file name :  ./Retina_Result/DRIVE_naive_attn_norm02_150000/sample_input_masks\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=240x432 at 0x7F918EC8E668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_sample = min(patches_imgs_train.shape[0],40)\n",
    "visualize(group_images(patches_imgs_train[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_imgs\")#.show()\n",
    "visualize(group_images(patches_masks_train[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks\")#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Retina_Result/DRIVE_naive_attn_norm02_150000DRIVE_naive_attn_norm02_150000_model.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### data generator test\n",
    "\n",
    "\n",
    "'./'+save_folder+'/'+name_experiment +name_experiment+ '_model.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 48, 48)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 48, 48)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 48, 48)   192         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 48, 48)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 48, 48)   9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 48, 48)   192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 48, 48)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_1 (SpatialDro (None, 32, 48, 48)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 48, 48)    0           spatial_dropout2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 48, 48)    0           spatial_dropout2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 48, 48)    0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 1, 48, 48)    98          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 32, 48, 48)   0           spatial_dropout2d_1[0][0]        \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 24, 24)   0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 24, 24)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 24, 24)   96          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 24, 24)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 24, 24)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 24, 24)   96          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 24, 24)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_2 (SpatialDro (None, 64, 24, 24)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 24, 24)    0           spatial_dropout2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1, 24, 24)    0           spatial_dropout2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2, 24, 24)    0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 1, 24, 24)    98          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 64, 24, 24)   0           spatial_dropout2d_2[0][0]        \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 12, 12)   0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 12, 12)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 12, 12)  48          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 12, 12)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 12, 12)  147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 12, 12)  48          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 12, 12)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_3 (SpatialDro (None, 128, 12, 12)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1, 12, 12)    0           spatial_dropout2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1, 12, 12)    0           spatial_dropout2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2, 12, 12)    0           lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 1, 12, 12)    98          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 128, 12, 12)  0           spatial_dropout2d_3[0][0]        \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 128, 24, 24)  0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 192, 24, 24)  0           multiply_2[0][0]                 \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 24, 24)   110656      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 24, 24)   96          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 24, 24)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 24, 24)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 24, 24)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 24, 24)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1, 24, 24)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1, 24, 24)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 2, 24, 24)    0           lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 1, 24, 24)    98          concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 64, 24, 24)   0           activation_8[0][0]               \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 48, 48)   0           multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 96, 48, 48)   0           multiply_1[0][0]                 \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 48, 48)   27680       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 48, 48)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 48, 48)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 48, 48)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 48, 48)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 48, 48)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1, 48, 48)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 1, 48, 48)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 2, 48, 48)    0           lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 1, 48, 48)    98          concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 32, 48, 48)   0           activation_10[0][0]              \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 2, 48, 48)    66          multiply_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2, 2304)      0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 2304, 2)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 2304, 2)      0           permute_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 472,748\n",
      "Trainable params: 472,124\n",
      "Non-trainable params: 624\n",
      "__________________________________________________________________________________________________\n",
      "Check: final output of the network:\n",
      "(None, 2304, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28253"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ch = patches_imgs_train.shape[1]\n",
    "patch_height = patches_imgs_train.shape[2]\n",
    "patch_width = patches_imgs_train.shape[3]\n",
    "\n",
    "model = naive_attn_unet(n_ch, patch_height, patch_width)  #the U-net model\n",
    "print (\"Check: final output of the network:\")\n",
    "print (model.output_shape)\n",
    "\n",
    "plot(model, to_file= './'+save_folder+'/'+name_experiment+'/' +name_experiment+ '_model.png')   #check how the model looks like\n",
    "#plot(model, to_file= name_experiment+'/'+name_experiment + '_model.png')   #check how the model looks like\n",
    "\n",
    "json_string = model.to_json()\n",
    "open('./'+save_folder+'/'+name_experiment+'/' +name_experiment+'_architecture.json', 'w').write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    }
   ],
   "source": [
    "print(int(config.get('training settings', 'num_subimgs')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training session] before mask unet func patch mask shape :  (150000, 1, 48, 48)\n",
      "[training session] after mask unet func patch mask shape :  (150000, 2304, 2)\n",
      "Train on 120000 samples, validate on 30000 samples\n",
      "Epoch 1/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.2821 - dice_coef: 0.8294 - val_loss: 0.1700 - val_dice_coef: 0.8996\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17003, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 2/150\n",
      "120000/120000 [==============================] - 213s 2ms/step - loss: 0.1807 - dice_coef: 0.8911 - val_loss: 0.1551 - val_dice_coef: 0.9050\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17003 to 0.15508, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 3/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1717 - dice_coef: 0.8962 - val_loss: 0.1508 - val_dice_coef: 0.9043\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15508 to 0.15078, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 4/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1679 - dice_coef: 0.8982 - val_loss: 0.1495 - val_dice_coef: 0.9112\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15078 to 0.14948, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 5/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1652 - dice_coef: 0.8995 - val_loss: 0.1470 - val_dice_coef: 0.9057\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.14948 to 0.14701, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 6/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1633 - dice_coef: 0.9005 - val_loss: 0.1452 - val_dice_coef: 0.9069\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.14701 to 0.14517, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 7/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1590 - dice_coef: 0.9034 - val_loss: 0.1235 - val_dice_coef: 0.9320\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.14517 to 0.12351, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 8/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1355 - dice_coef: 0.9230 - val_loss: 0.1167 - val_dice_coef: 0.9348\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12351 to 0.11669, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 9/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1322 - dice_coef: 0.9253 - val_loss: 0.1179 - val_dice_coef: 0.9376\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11669\n",
      "Epoch 10/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1301 - dice_coef: 0.9265 - val_loss: 0.1133 - val_dice_coef: 0.9360\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11669 to 0.11330, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 11/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1283 - dice_coef: 0.9276 - val_loss: 0.1124 - val_dice_coef: 0.9368\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11330 to 0.11242, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 12/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1269 - dice_coef: 0.9284 - val_loss: 0.1109 - val_dice_coef: 0.9351\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11242 to 0.11092, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 13/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1256 - dice_coef: 0.9291 - val_loss: 0.1102 - val_dice_coef: 0.9365\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11092 to 0.11019, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 14/150\n",
      "120000/120000 [==============================] - 214s 2ms/step - loss: 0.1243 - dice_coef: 0.9299 - val_loss: 0.1112 - val_dice_coef: 0.9393\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11019\n",
      "Epoch 15/150\n",
      "120000/120000 [==============================] - 213s 2ms/step - loss: 0.1233 - dice_coef: 0.9305 - val_loss: 0.1092 - val_dice_coef: 0.9394\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11019 to 0.10919, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 16/150\n",
      "120000/120000 [==============================] - 213s 2ms/step - loss: 0.1222 - dice_coef: 0.9311 - val_loss: 0.1078 - val_dice_coef: 0.9394\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.10919 to 0.10784, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 17/150\n",
      "120000/120000 [==============================] - 213s 2ms/step - loss: 0.1213 - dice_coef: 0.9315 - val_loss: 0.1066 - val_dice_coef: 0.9383\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.10784 to 0.10656, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 18/150\n",
      "120000/120000 [==============================] - 213s 2ms/step - loss: 0.1204 - dice_coef: 0.9321 - val_loss: 0.1060 - val_dice_coef: 0.9381\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10656 to 0.10595, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 19/150\n",
      "120000/120000 [==============================] - 212s 2ms/step - loss: 0.1196 - dice_coef: 0.9325 - val_loss: 0.1056 - val_dice_coef: 0.9388\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.10595 to 0.10562, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 20/150\n",
      "120000/120000 [==============================] - 212s 2ms/step - loss: 0.1188 - dice_coef: 0.9330 - val_loss: 0.1056 - val_dice_coef: 0.9416\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10562\n",
      "Epoch 21/150\n",
      "120000/120000 [==============================] - 212s 2ms/step - loss: 0.1181 - dice_coef: 0.9334 - val_loss: 0.1047 - val_dice_coef: 0.9401\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10562 to 0.10475, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 22/150\n",
      "120000/120000 [==============================] - 212s 2ms/step - loss: 0.1174 - dice_coef: 0.9337 - val_loss: 0.1046 - val_dice_coef: 0.9406\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10475 to 0.10461, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 23/150\n",
      "120000/120000 [==============================] - 211s 2ms/step - loss: 0.1167 - dice_coef: 0.9341 - val_loss: 0.1049 - val_dice_coef: 0.9421\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10461\n",
      "Epoch 24/150\n",
      "120000/120000 [==============================] - 211s 2ms/step - loss: 0.1161 - dice_coef: 0.9345 - val_loss: 0.1025 - val_dice_coef: 0.9403\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10461 to 0.10250, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 25/150\n",
      "120000/120000 [==============================] - 211s 2ms/step - loss: 0.1154 - dice_coef: 0.9348 - val_loss: 0.1032 - val_dice_coef: 0.9411\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10250\n",
      "Epoch 26/150\n",
      "120000/120000 [==============================] - 211s 2ms/step - loss: 0.1149 - dice_coef: 0.9351 - val_loss: 0.1045 - val_dice_coef: 0.9434\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10250\n",
      "Epoch 27/150\n",
      "120000/120000 [==============================] - 211s 2ms/step - loss: 0.1144 - dice_coef: 0.9354 - val_loss: 0.1020 - val_dice_coef: 0.9417\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10250 to 0.10204, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 28/150\n",
      "120000/120000 [==============================] - 210s 2ms/step - loss: 0.1138 - dice_coef: 0.9357 - val_loss: 0.1009 - val_dice_coef: 0.9419\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10204 to 0.10087, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 29/150\n",
      "120000/120000 [==============================] - 210s 2ms/step - loss: 0.1133 - dice_coef: 0.9360 - val_loss: 0.1012 - val_dice_coef: 0.9407\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10087\n",
      "Epoch 30/150\n",
      "120000/120000 [==============================] - 210s 2ms/step - loss: 0.1129 - dice_coef: 0.9362 - val_loss: 0.1009 - val_dice_coef: 0.9429\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10087\n",
      "Epoch 31/150\n",
      "120000/120000 [==============================] - 210s 2ms/step - loss: 0.1125 - dice_coef: 0.9364 - val_loss: 0.1003 - val_dice_coef: 0.9432\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10087 to 0.10035, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 32/150\n",
      "120000/120000 [==============================] - 210s 2ms/step - loss: 0.1121 - dice_coef: 0.9366 - val_loss: 0.1002 - val_dice_coef: 0.9428\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.10035 to 0.10016, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 33/150\n",
      "120000/120000 [==============================] - 210s 2ms/step - loss: 0.1117 - dice_coef: 0.9368 - val_loss: 0.1002 - val_dice_coef: 0.9443\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10016\n",
      "Epoch 34/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1114 - dice_coef: 0.9370 - val_loss: 0.0999 - val_dice_coef: 0.9429\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10016 to 0.09986, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 35/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1110 - dice_coef: 0.9372 - val_loss: 0.0997 - val_dice_coef: 0.9425\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09986 to 0.09968, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 36/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1107 - dice_coef: 0.9374 - val_loss: 0.1000 - val_dice_coef: 0.9426\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09968\n",
      "Epoch 37/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1104 - dice_coef: 0.9375 - val_loss: 0.0996 - val_dice_coef: 0.9443\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09968 to 0.09959, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 38/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1101 - dice_coef: 0.9377 - val_loss: 0.0993 - val_dice_coef: 0.9435\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09959 to 0.09931, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 39/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1098 - dice_coef: 0.9378 - val_loss: 0.0995 - val_dice_coef: 0.9437\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09931\n",
      "Epoch 40/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1095 - dice_coef: 0.9380 - val_loss: 0.0995 - val_dice_coef: 0.9438\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09931\n",
      "Epoch 41/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1092 - dice_coef: 0.9381 - val_loss: 0.0984 - val_dice_coef: 0.9438\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09931 to 0.09839, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 42/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1090 - dice_coef: 0.9382 - val_loss: 0.0985 - val_dice_coef: 0.9431\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09839\n",
      "Epoch 43/150\n",
      "120000/120000 [==============================] - 209s 2ms/step - loss: 0.1088 - dice_coef: 0.9384 - val_loss: 0.0984 - val_dice_coef: 0.9438\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09839 to 0.09837, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 44/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1085 - dice_coef: 0.9385 - val_loss: 0.0989 - val_dice_coef: 0.9434\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09837\n",
      "Epoch 45/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1083 - dice_coef: 0.9386 - val_loss: 0.0984 - val_dice_coef: 0.9436\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09837\n",
      "Epoch 46/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1080 - dice_coef: 0.9387 - val_loss: 0.0991 - val_dice_coef: 0.9448\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09837\n",
      "Epoch 47/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1079 - dice_coef: 0.9388 - val_loss: 0.0989 - val_dice_coef: 0.9429\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09837\n",
      "Epoch 48/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1076 - dice_coef: 0.9390 - val_loss: 0.0986 - val_dice_coef: 0.9437\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09837\n",
      "Epoch 49/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1074 - dice_coef: 0.9391 - val_loss: 0.0986 - val_dice_coef: 0.9446\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09837\n",
      "Epoch 50/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1072 - dice_coef: 0.9392 - val_loss: 0.0987 - val_dice_coef: 0.9444\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09837\n",
      "Epoch 51/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1070 - dice_coef: 0.9393 - val_loss: 0.0979 - val_dice_coef: 0.9450\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09837 to 0.09790, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 52/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1068 - dice_coef: 0.9394 - val_loss: 0.0985 - val_dice_coef: 0.9446\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09790\n",
      "Epoch 53/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1066 - dice_coef: 0.9395 - val_loss: 0.0980 - val_dice_coef: 0.9446\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09790\n",
      "Epoch 54/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1065 - dice_coef: 0.9396 - val_loss: 0.1012 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.09790\n",
      "Epoch 55/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1063 - dice_coef: 0.9397 - val_loss: 0.0982 - val_dice_coef: 0.9448\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09790\n",
      "Epoch 56/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1061 - dice_coef: 0.9398 - val_loss: 0.0994 - val_dice_coef: 0.9450\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09790\n",
      "Epoch 57/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1059 - dice_coef: 0.9399 - val_loss: 0.0987 - val_dice_coef: 0.9448\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.09790\n",
      "Epoch 58/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1058 - dice_coef: 0.9399 - val_loss: 0.0977 - val_dice_coef: 0.9441\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09790 to 0.09771, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 59/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1057 - dice_coef: 0.9400 - val_loss: 0.0977 - val_dice_coef: 0.9428\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09771 to 0.09769, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 60/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1054 - dice_coef: 0.9402 - val_loss: 0.0976 - val_dice_coef: 0.9444\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09769 to 0.09763, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 61/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1053 - dice_coef: 0.9402 - val_loss: 0.0980 - val_dice_coef: 0.9453\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.09763\n",
      "Epoch 62/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1051 - dice_coef: 0.9403 - val_loss: 0.0972 - val_dice_coef: 0.9443\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09763 to 0.09718, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 63/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1049 - dice_coef: 0.9404 - val_loss: 0.0969 - val_dice_coef: 0.9440\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09718 to 0.09686, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 64/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1047 - dice_coef: 0.9405 - val_loss: 0.0975 - val_dice_coef: 0.9449\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.09686\n",
      "Epoch 65/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1047 - dice_coef: 0.9405 - val_loss: 0.0978 - val_dice_coef: 0.9449\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.09686\n",
      "Epoch 66/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1045 - dice_coef: 0.9406 - val_loss: 0.0980 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.09686\n",
      "Epoch 67/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1044 - dice_coef: 0.9407 - val_loss: 0.0983 - val_dice_coef: 0.9450\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.09686\n",
      "Epoch 68/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1042 - dice_coef: 0.9408 - val_loss: 0.0973 - val_dice_coef: 0.9439\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09686\n",
      "Epoch 69/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1040 - dice_coef: 0.9409 - val_loss: 0.0977 - val_dice_coef: 0.9453\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.09686\n",
      "Epoch 70/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1039 - dice_coef: 0.9409 - val_loss: 0.0970 - val_dice_coef: 0.9444\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.09686\n",
      "Epoch 71/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1038 - dice_coef: 0.9410 - val_loss: 0.0974 - val_dice_coef: 0.9449\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.09686\n",
      "Epoch 72/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1036 - dice_coef: 0.9410 - val_loss: 0.0967 - val_dice_coef: 0.9442\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.09686 to 0.09668, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 73/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1035 - dice_coef: 0.9411 - val_loss: 0.0973 - val_dice_coef: 0.9452\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.09668\n",
      "Epoch 74/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1034 - dice_coef: 0.9412 - val_loss: 0.0972 - val_dice_coef: 0.9445\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.09668\n",
      "Epoch 75/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1032 - dice_coef: 0.9413 - val_loss: 0.0976 - val_dice_coef: 0.9435\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.09668\n",
      "Epoch 76/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1031 - dice_coef: 0.9413 - val_loss: 0.0969 - val_dice_coef: 0.9448\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.09668\n",
      "Epoch 77/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1030 - dice_coef: 0.9414 - val_loss: 0.0971 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.09668\n",
      "Epoch 78/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1029 - dice_coef: 0.9414 - val_loss: 0.0977 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.09668\n",
      "Epoch 79/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1028 - dice_coef: 0.9415 - val_loss: 0.0971 - val_dice_coef: 0.9436\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.09668\n",
      "Epoch 80/150\n",
      "120000/120000 [==============================] - 208s 2ms/step - loss: 0.1026 - dice_coef: 0.9416 - val_loss: 0.0970 - val_dice_coef: 0.9447\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.09668\n",
      "Epoch 81/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1025 - dice_coef: 0.9416 - val_loss: 0.0988 - val_dice_coef: 0.9455\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.09668\n",
      "Epoch 82/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1024 - dice_coef: 0.9417 - val_loss: 0.0971 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.09668\n",
      "Epoch 83/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1023 - dice_coef: 0.9418 - val_loss: 0.0966 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09668 to 0.09663, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 84/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1022 - dice_coef: 0.9418 - val_loss: 0.0972 - val_dice_coef: 0.9453\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.09663\n",
      "Epoch 85/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1020 - dice_coef: 0.9419 - val_loss: 0.0974 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.09663\n",
      "Epoch 86/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1020 - dice_coef: 0.9419 - val_loss: 0.0967 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.09663\n",
      "Epoch 87/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1018 - dice_coef: 0.9420 - val_loss: 0.0980 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.09663\n",
      "Epoch 88/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1017 - dice_coef: 0.9421 - val_loss: 0.0974 - val_dice_coef: 0.9449\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.09663\n",
      "Epoch 89/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1016 - dice_coef: 0.9421 - val_loss: 0.0971 - val_dice_coef: 0.9455\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.09663\n",
      "Epoch 90/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1015 - dice_coef: 0.9422 - val_loss: 0.0970 - val_dice_coef: 0.9442\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.09663\n",
      "Epoch 91/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1013 - dice_coef: 0.9423 - val_loss: 0.0972 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.09663\n",
      "Epoch 92/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1013 - dice_coef: 0.9422 - val_loss: 0.0966 - val_dice_coef: 0.9450\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.09663 to 0.09662, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 93/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1012 - dice_coef: 0.9423 - val_loss: 0.0976 - val_dice_coef: 0.9456\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.09662\n",
      "Epoch 94/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1011 - dice_coef: 0.9424 - val_loss: 0.0973 - val_dice_coef: 0.9456\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.09662\n",
      "Epoch 95/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1010 - dice_coef: 0.9424 - val_loss: 0.0970 - val_dice_coef: 0.9454\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.09662\n",
      "Epoch 96/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1009 - dice_coef: 0.9425 - val_loss: 0.0968 - val_dice_coef: 0.9462\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.09662\n",
      "Epoch 97/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1008 - dice_coef: 0.9426 - val_loss: 0.0970 - val_dice_coef: 0.9447\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.09662\n",
      "Epoch 98/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1006 - dice_coef: 0.9426 - val_loss: 0.0975 - val_dice_coef: 0.9461\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.09662\n",
      "Epoch 99/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1006 - dice_coef: 0.9426 - val_loss: 0.0973 - val_dice_coef: 0.9454\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.09662\n",
      "Epoch 100/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1004 - dice_coef: 0.9427 - val_loss: 0.0959 - val_dice_coef: 0.9444\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.09662 to 0.09589, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 101/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.1004 - dice_coef: 0.9428 - val_loss: 0.0968 - val_dice_coef: 0.9453\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.09589\n",
      "Epoch 102/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.1003 - dice_coef: 0.9428 - val_loss: 0.0975 - val_dice_coef: 0.9461\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.09589\n",
      "Epoch 103/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.1001 - dice_coef: 0.9429 - val_loss: 0.0969 - val_dice_coef: 0.9455\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.09589\n",
      "Epoch 104/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.1001 - dice_coef: 0.9429 - val_loss: 0.0968 - val_dice_coef: 0.9456\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.09589\n",
      "Epoch 105/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.1000 - dice_coef: 0.9430 - val_loss: 0.0971 - val_dice_coef: 0.9463\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.09589\n",
      "Epoch 106/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0999 - dice_coef: 0.9430 - val_loss: 0.0967 - val_dice_coef: 0.9460\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.09589\n",
      "Epoch 107/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0998 - dice_coef: 0.9431 - val_loss: 0.0970 - val_dice_coef: 0.9454\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.09589\n",
      "Epoch 108/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0997 - dice_coef: 0.9431 - val_loss: 0.0968 - val_dice_coef: 0.9453\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.09589\n",
      "Epoch 109/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0996 - dice_coef: 0.9431 - val_loss: 0.0974 - val_dice_coef: 0.9458\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.09589\n",
      "Epoch 110/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0995 - dice_coef: 0.9432 - val_loss: 0.0963 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.09589\n",
      "Epoch 111/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0994 - dice_coef: 0.9433 - val_loss: 0.0961 - val_dice_coef: 0.9449\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.09589\n",
      "Epoch 112/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0993 - dice_coef: 0.9433 - val_loss: 0.0970 - val_dice_coef: 0.9453\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.09589\n",
      "Epoch 113/150\n",
      "120000/120000 [==============================] - 207s 2ms/step - loss: 0.0992 - dice_coef: 0.9434 - val_loss: 0.0968 - val_dice_coef: 0.9459\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.09589\n",
      "Epoch 114/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0992 - dice_coef: 0.9434 - val_loss: 0.0986 - val_dice_coef: 0.9469\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.09589\n",
      "Epoch 115/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0991 - dice_coef: 0.9434 - val_loss: 0.0963 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.09589\n",
      "Epoch 116/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0990 - dice_coef: 0.9435 - val_loss: 0.0974 - val_dice_coef: 0.9466\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.09589\n",
      "Epoch 117/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0989 - dice_coef: 0.9435 - val_loss: 0.0958 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09589 to 0.09577, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 118/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0988 - dice_coef: 0.9436 - val_loss: 0.0969 - val_dice_coef: 0.9462\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.09577\n",
      "Epoch 119/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0988 - dice_coef: 0.9436 - val_loss: 0.0985 - val_dice_coef: 0.9462\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.09577\n",
      "Epoch 120/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0986 - dice_coef: 0.9436 - val_loss: 0.0970 - val_dice_coef: 0.9461\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.09577\n",
      "Epoch 121/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0986 - dice_coef: 0.9436 - val_loss: 0.0969 - val_dice_coef: 0.9455\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.09577\n",
      "Epoch 122/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0985 - dice_coef: 0.9437 - val_loss: 0.0966 - val_dice_coef: 0.9448\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.09577\n",
      "Epoch 123/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0984 - dice_coef: 0.9438 - val_loss: 0.0972 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.09577\n",
      "Epoch 124/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0983 - dice_coef: 0.9438 - val_loss: 0.0966 - val_dice_coef: 0.9459\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.09577\n",
      "Epoch 125/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0983 - dice_coef: 0.9438 - val_loss: 0.0967 - val_dice_coef: 0.9461\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.09577\n",
      "Epoch 126/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0982 - dice_coef: 0.9439 - val_loss: 0.0977 - val_dice_coef: 0.9460\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.09577\n",
      "Epoch 127/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0981 - dice_coef: 0.9439 - val_loss: 0.0969 - val_dice_coef: 0.9452\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.09577\n",
      "Epoch 128/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0981 - dice_coef: 0.9439 - val_loss: 0.0978 - val_dice_coef: 0.9466\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.09577\n",
      "Epoch 129/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0979 - dice_coef: 0.9440 - val_loss: 0.0981 - val_dice_coef: 0.9466\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.09577\n",
      "Epoch 130/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0979 - dice_coef: 0.9440 - val_loss: 0.0967 - val_dice_coef: 0.9464\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.09577\n",
      "Epoch 131/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0978 - dice_coef: 0.9441 - val_loss: 0.0969 - val_dice_coef: 0.9464\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.09577\n",
      "Epoch 132/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0978 - dice_coef: 0.9441 - val_loss: 0.0963 - val_dice_coef: 0.9462\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.09577\n",
      "Epoch 133/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0977 - dice_coef: 0.9441 - val_loss: 0.0972 - val_dice_coef: 0.9459\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.09577\n",
      "Epoch 134/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0975 - dice_coef: 0.9442 - val_loss: 0.0963 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.09577\n",
      "Epoch 135/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0975 - dice_coef: 0.9442 - val_loss: 0.0971 - val_dice_coef: 0.9460\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.09577\n",
      "Epoch 136/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0974 - dice_coef: 0.9443 - val_loss: 0.0968 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.09577\n",
      "Epoch 137/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0973 - dice_coef: 0.9443 - val_loss: 0.0955 - val_dice_coef: 0.9456\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.09577 to 0.09552, saving model to ./Retina_Result/DRIVE_naive_attn_norm02_150000/best_weights.h5\n",
      "Epoch 138/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0973 - dice_coef: 0.9443 - val_loss: 0.0966 - val_dice_coef: 0.9455\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.09552\n",
      "Epoch 139/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0972 - dice_coef: 0.9444 - val_loss: 0.0968 - val_dice_coef: 0.9454\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.09552\n",
      "Epoch 140/150\n",
      "120000/120000 [==============================] - 206s 2ms/step - loss: 0.0971 - dice_coef: 0.9444 - val_loss: 0.0978 - val_dice_coef: 0.9460\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.09552\n",
      "Epoch 141/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0971 - dice_coef: 0.9444 - val_loss: 0.0966 - val_dice_coef: 0.9467\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.09552\n",
      "Epoch 142/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0970 - dice_coef: 0.9445 - val_loss: 0.0966 - val_dice_coef: 0.9464\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.09552\n",
      "Epoch 143/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0969 - dice_coef: 0.9445 - val_loss: 0.0967 - val_dice_coef: 0.9456\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.09552\n",
      "Epoch 144/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0969 - dice_coef: 0.9445 - val_loss: 0.0957 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.09552\n",
      "Epoch 145/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0968 - dice_coef: 0.9446 - val_loss: 0.0962 - val_dice_coef: 0.9463\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.09552\n",
      "Epoch 146/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0968 - dice_coef: 0.9446 - val_loss: 0.0965 - val_dice_coef: 0.9451\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.09552\n",
      "Epoch 147/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0967 - dice_coef: 0.9446 - val_loss: 0.0971 - val_dice_coef: 0.9457\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.09552\n",
      "Epoch 148/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0966 - dice_coef: 0.9447 - val_loss: 0.0965 - val_dice_coef: 0.9463\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.09552\n",
      "Epoch 149/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0965 - dice_coef: 0.9447 - val_loss: 0.0959 - val_dice_coef: 0.9464\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.09552\n",
      "Epoch 150/150\n",
      "120000/120000 [==============================] - 205s 2ms/step - loss: 0.0964 - dice_coef: 0.9448 - val_loss: 0.0967 - val_dice_coef: 0.9465\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.09552\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Keras provides a set of functions called callbacks: \n",
    "you can think of callbacks as events that will be triggered at certain training states. \n",
    "The callback we need for checkpointing is the ModelCheckpoint \n",
    "which provides all the features we need according to the checkpointing strategy we adopted in our example\n",
    "'''\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./'+save_folder+'/'+name_experiment+'/best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
    "\n",
    "print('[training session] before mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "patches_masks_train = masks_Unet(patches_masks_train)  #reduce memory consumption\n",
    "print('[training session] after mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "\n",
    "\n",
    "history = model.fit(patches_imgs_train, patches_masks_train, epochs=num_epochs, batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.2, callbacks=[checkpointer])\n",
    "model.save_weights('./'+save_folder+'/'+name_experiment +'/last_weights.h5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dice_coef': [0.8294014417807262,\n",
       "  0.8910772324244182,\n",
       "  0.8961704160531362,\n",
       "  0.8981942489941915,\n",
       "  0.8995450358390809,\n",
       "  0.9005379984060923,\n",
       "  0.9034108185609182,\n",
       "  0.9230222499370575,\n",
       "  0.9252825016498566,\n",
       "  0.9265274269580841,\n",
       "  0.9275875656763712,\n",
       "  0.9283609973112742,\n",
       "  0.92914604900678,\n",
       "  0.9298571897506713,\n",
       "  0.9304580447832743,\n",
       "  0.9310520376841227,\n",
       "  0.9315481357574463,\n",
       "  0.9320531469027201,\n",
       "  0.9325222393671672,\n",
       "  0.9329711858908335,\n",
       "  0.9333588450431823,\n",
       "  0.9337331047375997,\n",
       "  0.9341445658683777,\n",
       "  0.9344699871857961,\n",
       "  0.9348386617342631,\n",
       "  0.9350905140399933,\n",
       "  0.9354266937732697,\n",
       "  0.9357040080388387,\n",
       "  0.9359615662733713,\n",
       "  0.9361703077952067,\n",
       "  0.9364026538530985,\n",
       "  0.9365766592979431,\n",
       "  0.9368104603449504,\n",
       "  0.9369904455343883,\n",
       "  0.9371859930038452,\n",
       "  0.9373533996105194,\n",
       "  0.9374979905605316,\n",
       "  0.9376695650259653,\n",
       "  0.9378441752433777,\n",
       "  0.9379799851576487,\n",
       "  0.9381076395670573,\n",
       "  0.9382359672864278,\n",
       "  0.938350866762797,\n",
       "  0.9384858837286632,\n",
       "  0.9385827768643697,\n",
       "  0.9387464841683706,\n",
       "  0.9388319738547007,\n",
       "  0.9389595193862915,\n",
       "  0.9390853299617767,\n",
       "  0.9391539664745331,\n",
       "  0.9392956650892893,\n",
       "  0.939371881866455,\n",
       "  0.9394906568050384,\n",
       "  0.9395561562379201,\n",
       "  0.9396722333431244,\n",
       "  0.9397767532984416,\n",
       "  0.9398720141251882,\n",
       "  0.9399314020474752,\n",
       "  0.9399950614611308,\n",
       "  0.940164051135381,\n",
       "  0.9402038338979085,\n",
       "  0.9402696566740671,\n",
       "  0.9403686848640442,\n",
       "  0.9404879633108775,\n",
       "  0.9405132779757182,\n",
       "  0.9406052899837494,\n",
       "  0.9406862326939901,\n",
       "  0.9407566124121348,\n",
       "  0.9408500282128652,\n",
       "  0.9409103629430136,\n",
       "  0.9409982342561086,\n",
       "  0.9410486210664113,\n",
       "  0.9411333807309469,\n",
       "  0.9412050720373789,\n",
       "  0.9412542215983073,\n",
       "  0.9413259547869365,\n",
       "  0.9414153048992157,\n",
       "  0.9414345586458842,\n",
       "  0.9415059517542521,\n",
       "  0.9415846322536469,\n",
       "  0.9416271712938945,\n",
       "  0.9417013865947723,\n",
       "  0.9417723572413127,\n",
       "  0.9418017930984497,\n",
       "  0.9418853113651275,\n",
       "  0.9419401383717855,\n",
       "  0.9420155785242716,\n",
       "  0.9420723228772482,\n",
       "  0.9421208115577697,\n",
       "  0.9421877224604289,\n",
       "  0.9422543074448904,\n",
       "  0.9422483223756154,\n",
       "  0.9423457819302877,\n",
       "  0.9423941537221273,\n",
       "  0.9424433396339417,\n",
       "  0.9424890692392985,\n",
       "  0.9425702181180318,\n",
       "  0.9426227868556977,\n",
       "  0.9426421643257141,\n",
       "  0.9427344723860422,\n",
       "  0.9427598966916402,\n",
       "  0.9427920525232951,\n",
       "  0.9428804844379425,\n",
       "  0.9428901099046071,\n",
       "  0.9429640690008799,\n",
       "  0.9429990795294444,\n",
       "  0.9430874385197957,\n",
       "  0.9430871305465698,\n",
       "  0.9431422237873077,\n",
       "  0.9431824532032013,\n",
       "  0.9432505877335866,\n",
       "  0.9432978717962901,\n",
       "  0.9433549217859905,\n",
       "  0.9433661161263783,\n",
       "  0.9434252500693003,\n",
       "  0.9434685914357503,\n",
       "  0.9435154988129933,\n",
       "  0.9435532751560212,\n",
       "  0.9435752218564352,\n",
       "  0.9436364131450653,\n",
       "  0.9436373336950938,\n",
       "  0.9437262454509735,\n",
       "  0.9437597189744313,\n",
       "  0.943802761554718,\n",
       "  0.9438348864078522,\n",
       "  0.9438855746587117,\n",
       "  0.943929900376002,\n",
       "  0.9439268654028574,\n",
       "  0.9439809783299764,\n",
       "  0.9440305450439453,\n",
       "  0.9440918287595114,\n",
       "  0.9440855363210042,\n",
       "  0.9441341961860656,\n",
       "  0.9441950926462809,\n",
       "  0.9442410466353098,\n",
       "  0.9442541346549987,\n",
       "  0.9443025219599406,\n",
       "  0.9443444998264313,\n",
       "  0.9443552058855692,\n",
       "  0.944407531261444,\n",
       "  0.9444266141573588,\n",
       "  0.9444840791543325,\n",
       "  0.9445110385258992,\n",
       "  0.9445307438373566,\n",
       "  0.9445741651852926,\n",
       "  0.9445935427029928,\n",
       "  0.9446263351917267,\n",
       "  0.9446625616232555,\n",
       "  0.944744026851654,\n",
       "  0.9447800206343333],\n",
       " 'loss': [0.28212340052525203,\n",
       "  0.18069216065804164,\n",
       "  0.171705435103178,\n",
       "  0.16786709338227906,\n",
       "  0.1651714054942131,\n",
       "  0.16327785655856134,\n",
       "  0.15900138746500014,\n",
       "  0.13545028961499533,\n",
       "  0.1321604412039121,\n",
       "  0.1300576748987039,\n",
       "  0.12826893480817478,\n",
       "  0.12694702565073968,\n",
       "  0.12556750770807265,\n",
       "  0.12430091400543848,\n",
       "  0.12327951587041219,\n",
       "  0.1222106074710687,\n",
       "  0.12132576725880305,\n",
       "  0.12042752487659454,\n",
       "  0.11961388925512632,\n",
       "  0.11879172526597977,\n",
       "  0.11809183931350709,\n",
       "  0.11740518464247386,\n",
       "  0.11669164308309556,\n",
       "  0.11606726643840472,\n",
       "  0.11541123000780741,\n",
       "  0.11492377559542656,\n",
       "  0.11435216484367848,\n",
       "  0.1138069316526254,\n",
       "  0.11330800939003627,\n",
       "  0.11292935973405838,\n",
       "  0.11247801557381948,\n",
       "  0.11214340759913127,\n",
       "  0.1117184178173542,\n",
       "  0.11135891070564588,\n",
       "  0.1110091851234436,\n",
       "  0.11068019233345985,\n",
       "  0.11040343675613404,\n",
       "  0.11007572720646858,\n",
       "  0.10976907080014547,\n",
       "  0.10949228311777115,\n",
       "  0.10923504463334878,\n",
       "  0.10902163763840993,\n",
       "  0.10876741382877032,\n",
       "  0.10853084817330043,\n",
       "  0.1083430966436863,\n",
       "  0.1080183441778024,\n",
       "  0.10787659550507864,\n",
       "  0.10761834295590719,\n",
       "  0.10740381813247998,\n",
       "  0.10723977918624877,\n",
       "  0.10699773503939311,\n",
       "  0.10683769705692928,\n",
       "  0.1066259384671847,\n",
       "  0.10648499284187952,\n",
       "  0.106271808608373,\n",
       "  0.1060844985961914,\n",
       "  0.10589639390011628,\n",
       "  0.1057740542391936,\n",
       "  0.10565291708906492,\n",
       "  0.10535028565724691,\n",
       "  0.10526141975720724,\n",
       "  0.10512530365983645,\n",
       "  0.10492800893584887,\n",
       "  0.10472929091850916,\n",
       "  0.10466001557509104,\n",
       "  0.10449516484340032,\n",
       "  0.10436380460858345,\n",
       "  0.10420495401918888,\n",
       "  0.10403003530601661,\n",
       "  0.10391720690528551,\n",
       "  0.10376931405564149,\n",
       "  0.10363946302433809,\n",
       "  0.10350963925917943,\n",
       "  0.10336920432051022,\n",
       "  0.10324285557468732,\n",
       "  0.1031108488758405,\n",
       "  0.10297426986098289,\n",
       "  0.10290093982120355,\n",
       "  0.10278843144476414,\n",
       "  0.10262261729935805,\n",
       "  0.102542585537831,\n",
       "  0.10241882158120473,\n",
       "  0.10229197797775269,\n",
       "  0.1022118812272946,\n",
       "  0.10204778420726458,\n",
       "  0.1019657238384088,\n",
       "  0.10180335147778193,\n",
       "  0.101694470034043,\n",
       "  0.10159019976059595,\n",
       "  0.10148017557660739,\n",
       "  0.10133787636359533,\n",
       "  0.10134270001848539,\n",
       "  0.10116327947080135,\n",
       "  0.10107714702884356,\n",
       "  0.10098340512514115,\n",
       "  0.10088832060496013,\n",
       "  0.10076894268194835,\n",
       "  0.1006340563138326,\n",
       "  0.10059403691093127,\n",
       "  0.10042716598113378,\n",
       "  0.10035409357448419,\n",
       "  0.10029963123897712,\n",
       "  0.10014349287350972,\n",
       "  0.10010925279557704,\n",
       "  0.0999676849802335,\n",
       "  0.09990111561914285,\n",
       "  0.09975899337331454,\n",
       "  0.09971405960222085,\n",
       "  0.09962944297194481,\n",
       "  0.09954043491284052,\n",
       "  0.09942581363022328,\n",
       "  0.09930404364367326,\n",
       "  0.0992257836262385,\n",
       "  0.09919743563036124,\n",
       "  0.09907407063245774,\n",
       "  0.09897859835326672,\n",
       "  0.09891420483191808,\n",
       "  0.09882210128903389,\n",
       "  0.09876434559822082,\n",
       "  0.09864126882950465,\n",
       "  0.09864522173802058,\n",
       "  0.09849101240038871,\n",
       "  0.09841356900831064,\n",
       "  0.09833698942263921,\n",
       "  0.09825853099326293,\n",
       "  0.09816520376304785,\n",
       "  0.09809274628361066,\n",
       "  0.09805655367473762,\n",
       "  0.09794357777635257,\n",
       "  0.09788577244679134,\n",
       "  0.09776891812582811,\n",
       "  0.09776173915763696,\n",
       "  0.09766940751075745,\n",
       "  0.09754953747491042,\n",
       "  0.09746678121387958,\n",
       "  0.09743034365375837,\n",
       "  0.09733321753342947,\n",
       "  0.09728213988443216,\n",
       "  0.09722824349403382,\n",
       "  0.09712746237417062,\n",
       "  0.09709189946055412,\n",
       "  0.09698443002402782,\n",
       "  0.09693229230244954,\n",
       "  0.09687562919457754,\n",
       "  0.09679641490280629,\n",
       "  0.09676486598749956,\n",
       "  0.09668355690638224,\n",
       "  0.09661106672684351,\n",
       "  0.0964786572754383,\n",
       "  0.09638214945693811],\n",
       " 'val_dice_coef': [0.8996048601150513,\n",
       "  0.9049778378168741,\n",
       "  0.9043383914947509,\n",
       "  0.9111845740954081,\n",
       "  0.9057056094169617,\n",
       "  0.9068566020329794,\n",
       "  0.9320350589434305,\n",
       "  0.9347790873845419,\n",
       "  0.9375744380950928,\n",
       "  0.9360170899073282,\n",
       "  0.9367819154103597,\n",
       "  0.9351014816602071,\n",
       "  0.9365146682103475,\n",
       "  0.939346186765035,\n",
       "  0.9393929989496866,\n",
       "  0.9394252593358358,\n",
       "  0.9382591766357422,\n",
       "  0.9380584981282553,\n",
       "  0.9388270902951559,\n",
       "  0.9415753041903178,\n",
       "  0.9400744276682536,\n",
       "  0.940644081624349,\n",
       "  0.9420514330863953,\n",
       "  0.9403004534403483,\n",
       "  0.9411404344558716,\n",
       "  0.9434201989491781,\n",
       "  0.9417127211888631,\n",
       "  0.9419353203137716,\n",
       "  0.940690726629893,\n",
       "  0.9429276030858358,\n",
       "  0.9431853624661763,\n",
       "  0.9427616388638814,\n",
       "  0.9442754910469056,\n",
       "  0.942925624624888,\n",
       "  0.9424596260706584,\n",
       "  0.9426138186772665,\n",
       "  0.9442517073631287,\n",
       "  0.9434772813479105,\n",
       "  0.9437431836128235,\n",
       "  0.9438454634348551,\n",
       "  0.9438377658208211,\n",
       "  0.9430849839528401,\n",
       "  0.9438467420260112,\n",
       "  0.943385897954305,\n",
       "  0.9436447340647379,\n",
       "  0.9448287316958109,\n",
       "  0.9428850270271302,\n",
       "  0.9437268703778585,\n",
       "  0.9445604039510092,\n",
       "  0.9443706093470255,\n",
       "  0.9450416899681091,\n",
       "  0.9445902111689249,\n",
       "  0.9445604817390442,\n",
       "  0.9456943690617879,\n",
       "  0.9447725350379944,\n",
       "  0.9449700611432393,\n",
       "  0.94478792552948,\n",
       "  0.944074540233612,\n",
       "  0.9427815346082051,\n",
       "  0.9444388586680095,\n",
       "  0.9453419890403748,\n",
       "  0.9443248801231384,\n",
       "  0.9440047656695048,\n",
       "  0.9449240197181702,\n",
       "  0.9448639929453532,\n",
       "  0.9456603907585144,\n",
       "  0.9449774319330851,\n",
       "  0.9439184307734172,\n",
       "  0.945274293422699,\n",
       "  0.9444043460210164,\n",
       "  0.9449061713218689,\n",
       "  0.9442033356984456,\n",
       "  0.945246860853831,\n",
       "  0.9445208843866985,\n",
       "  0.9434689383188883,\n",
       "  0.9447611236572265,\n",
       "  0.9451300478299459,\n",
       "  0.9451097140312195,\n",
       "  0.9435983931541443,\n",
       "  0.9447440744082133,\n",
       "  0.9455070777893066,\n",
       "  0.9450795132637024,\n",
       "  0.9450510686874389,\n",
       "  0.9453087275187174,\n",
       "  0.9451304478645325,\n",
       "  0.9450775949796041,\n",
       "  0.9450740228652954,\n",
       "  0.9449172995567322,\n",
       "  0.9455303051948547,\n",
       "  0.9442480252583821,\n",
       "  0.9457183028539021,\n",
       "  0.9449598297437032,\n",
       "  0.9455536659240723,\n",
       "  0.9456465585072835,\n",
       "  0.9453719202041626,\n",
       "  0.9462115430196126,\n",
       "  0.9447059724171957,\n",
       "  0.946056273206075,\n",
       "  0.9454053502082824,\n",
       "  0.9443681018511454,\n",
       "  0.9453002782185872,\n",
       "  0.9461234559059143,\n",
       "  0.9455168066660563,\n",
       "  0.9455655149777731,\n",
       "  0.9462527916590373,\n",
       "  0.9460004832585652,\n",
       "  0.9453736009279887,\n",
       "  0.9453383806546529,\n",
       "  0.9457768945376078,\n",
       "  0.9456819835344951,\n",
       "  0.9448787288665772,\n",
       "  0.9453394453048706,\n",
       "  0.9459075101852417,\n",
       "  0.9469264071464538,\n",
       "  0.9457490985234579,\n",
       "  0.9466263012250264,\n",
       "  0.9450956809043884,\n",
       "  0.9462421847661336,\n",
       "  0.9461765506108601,\n",
       "  0.946138986269633,\n",
       "  0.9455223464330037,\n",
       "  0.9448073734283448,\n",
       "  0.9457386962890625,\n",
       "  0.9459148739814758,\n",
       "  0.9461433139165243,\n",
       "  0.9460117858568827,\n",
       "  0.9452127229690552,\n",
       "  0.9465798493385315,\n",
       "  0.9466091406822205,\n",
       "  0.9463722823778788,\n",
       "  0.9463710097312927,\n",
       "  0.946238272635142,\n",
       "  0.945860940138499,\n",
       "  0.945698642539978,\n",
       "  0.9459859097162883,\n",
       "  0.945705850760142,\n",
       "  0.9455545913060506,\n",
       "  0.9455454949696859,\n",
       "  0.9453528010686238,\n",
       "  0.9460214273134867,\n",
       "  0.9467363181432088,\n",
       "  0.946394590028127,\n",
       "  0.9456124698956807,\n",
       "  0.9456624335606892,\n",
       "  0.9463177807807922,\n",
       "  0.9451168809890748,\n",
       "  0.9457186430295308,\n",
       "  0.9463421304702759,\n",
       "  0.9463930740356445,\n",
       "  0.9465421671231588],\n",
       " 'val_loss': [0.1700313328663508,\n",
       "  0.15507870235443116,\n",
       "  0.15077955000797907,\n",
       "  0.14948215007781981,\n",
       "  0.14700926558176677,\n",
       "  0.14516539646784465,\n",
       "  0.12351055347919464,\n",
       "  0.11668907407124837,\n",
       "  0.1179123996814092,\n",
       "  0.11330380547444026,\n",
       "  0.11241999466021856,\n",
       "  0.11091754611333211,\n",
       "  0.110192085313797,\n",
       "  0.11122299970785776,\n",
       "  0.10919190634886423,\n",
       "  0.10784446809689203,\n",
       "  0.10655832217534383,\n",
       "  0.10595320582787196,\n",
       "  0.10562250497341157,\n",
       "  0.10563415946563084,\n",
       "  0.10474832057555516,\n",
       "  0.10460634280443191,\n",
       "  0.10486775576670965,\n",
       "  0.10249835899273554,\n",
       "  0.10320831901232401,\n",
       "  0.1045004836042722,\n",
       "  0.10203540930747985,\n",
       "  0.10087293860514959,\n",
       "  0.10119063781897226,\n",
       "  0.10089841055472692,\n",
       "  0.10034980905056,\n",
       "  0.10015501783688863,\n",
       "  0.10015801316897074,\n",
       "  0.09985632491111755,\n",
       "  0.09968248402674992,\n",
       "  0.09995885135332744,\n",
       "  0.09959235194921494,\n",
       "  0.09930866796573003,\n",
       "  0.0994679093281428,\n",
       "  0.0995275029261907,\n",
       "  0.09839403235514958,\n",
       "  0.0985050307114919,\n",
       "  0.09836734073162079,\n",
       "  0.09891571097373962,\n",
       "  0.09838922990163167,\n",
       "  0.09908450943628946,\n",
       "  0.09885891577005386,\n",
       "  0.09856338860591253,\n",
       "  0.09855936833222707,\n",
       "  0.09870188602209091,\n",
       "  0.09789799758593241,\n",
       "  0.09849717609882355,\n",
       "  0.09799155743916829,\n",
       "  0.10124598652919134,\n",
       "  0.09820812759399414,\n",
       "  0.09943560488621393,\n",
       "  0.09872336520353953,\n",
       "  0.09771014993190766,\n",
       "  0.09768940606514613,\n",
       "  0.097631769657135,\n",
       "  0.09795424561500549,\n",
       "  0.09717609848976136,\n",
       "  0.09686028031508127,\n",
       "  0.0975347449461619,\n",
       "  0.09780727657874426,\n",
       "  0.09798882214228312,\n",
       "  0.09830472518205642,\n",
       "  0.09728627101580302,\n",
       "  0.09774711433251698,\n",
       "  0.09700347000757853,\n",
       "  0.09738238190015157,\n",
       "  0.09668439919551214,\n",
       "  0.09734599872032801,\n",
       "  0.09723430060148239,\n",
       "  0.09760228095849355,\n",
       "  0.09690061890681585,\n",
       "  0.09713860410054524,\n",
       "  0.09766014856100082,\n",
       "  0.09713486570517223,\n",
       "  0.09698875386317571,\n",
       "  0.09877905018726985,\n",
       "  0.0971339677810669,\n",
       "  0.09662541977564494,\n",
       "  0.09723083990812302,\n",
       "  0.0974413180033366,\n",
       "  0.09669580986102422,\n",
       "  0.0980466642300288,\n",
       "  0.09736619207859039,\n",
       "  0.09709704736868541,\n",
       "  0.09697608710924785,\n",
       "  0.0972170414129893,\n",
       "  0.0966240650733312,\n",
       "  0.09761722270647685,\n",
       "  0.09725350426832835,\n",
       "  0.09698432066043218,\n",
       "  0.09681011563539504,\n",
       "  0.09702815916538239,\n",
       "  0.09753085868755976,\n",
       "  0.09730558104117712,\n",
       "  0.0958860034942627,\n",
       "  0.09678932563066482,\n",
       "  0.09754086281458536,\n",
       "  0.09688614248434703,\n",
       "  0.09681919443209966,\n",
       "  0.09709932217995326,\n",
       "  0.09668706209659576,\n",
       "  0.09700650905768077,\n",
       "  0.0968070650935173,\n",
       "  0.09744537211259206,\n",
       "  0.09628814593950907,\n",
       "  0.09605473461945851,\n",
       "  0.09700762559175491,\n",
       "  0.09680292241970698,\n",
       "  0.09857359643777211,\n",
       "  0.09628184308608373,\n",
       "  0.09736752212047577,\n",
       "  0.09577152761220932,\n",
       "  0.09686061680714289,\n",
       "  0.09846322756608328,\n",
       "  0.09699301695426304,\n",
       "  0.09689223699967066,\n",
       "  0.0965682015856107,\n",
       "  0.097234008427461,\n",
       "  0.09663212319612503,\n",
       "  0.0966573566476504,\n",
       "  0.09769298479557037,\n",
       "  0.0968851122101148,\n",
       "  0.09777024532556534,\n",
       "  0.09811258393526077,\n",
       "  0.09667513449192047,\n",
       "  0.09685762193202972,\n",
       "  0.09628541646798451,\n",
       "  0.0971648704926173,\n",
       "  0.09630971568425496,\n",
       "  0.09708867792288463,\n",
       "  0.09683170173565547,\n",
       "  0.09551858586867651,\n",
       "  0.09656253046194713,\n",
       "  0.09678202868302663,\n",
       "  0.09780227688153585,\n",
       "  0.09662031767368316,\n",
       "  0.09663645193179449,\n",
       "  0.09673041266202927,\n",
       "  0.09568614501158397,\n",
       "  0.09622664234638215,\n",
       "  0.09650849224328995,\n",
       "  0.09706093826293945,\n",
       "  0.09647695441246033,\n",
       "  0.09588727941513062,\n",
       "  0.09674240446488062]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss= history.history['val_loss']\n",
    "acc = history.history['dice_coef']\n",
    "val_acc = history.history['val_dice_coef']\n",
    "\n",
    "epochs = range(1,len(acc) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_loss_result.png\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label = \"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, 'r', label = \"Validation accuracy\")\n",
    "plt.title(\"Training and Validation acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_acc_result.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
